<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Ubuntu Tmux 启用鼠标滚动]]></title>
    <url>%2F2018%2F09%2F12%2FUbuntu%20Tmux%20%E5%90%AF%E7%94%A8%E9%BC%A0%E6%A0%87%E6%BB%9A%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[Ubuntu Tmux 启用鼠标滚动在Ubuntu上使用Tmux是一件非常舒服的事，但有时使用鼠标滚轮时，和平时使用终端的习惯不怎么一致，因此可以设置启用鼠标滚轮。具体方式：按完前缀ctrl+B后，再按分号：进入命令行模式，输入以下命令：1set -g mouse on 就启用了鼠标滚轮，可以通过鼠标直接选择不同的窗口，也可以上下直接翻页。 Tip但在以上设置下，会发现无法用中键向 tmux 中复制文本，也无法将 tmux 中选择好的文本中键复制到系统其他应用程序中。这里有一个 trick，那就是在 tmux 中不论选择还是复制时，都按住 Shift 键，你会发现熟悉的中键又回来了 :-) 此外，还可以使用 Shift+Insert 快捷键将系统剪切板中的内容输入 tmux 中。 相对于 tmux 原生的选择模式（不加 shift 键），使用系统选择有个缺陷，即当一行内存在多个面板时，无法选择单个面板中的内容，这时就必须使用 tmux 自带的复制粘贴系统了。 参考：https://superuser.com/questions/210125/scroll-shell-output-with-mouse-in-tmux,http://mindonmind.github.io/notes/linux/tmux_copy_paste_by_mouse.html里面有人说输入setw -g mouse on,但我没有成功，我的ubuntu版本为14.04。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>linux</tag>
        <tag>tmux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Splunk-SDK-Python学习]]></title>
    <url>%2F2018%2F09%2F12%2FSplunk-SDK-Python%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[Splunk最近在公司实习用到了splunk,需要调用splunk sdk进行一些简单的开发，目前把我这个星期的一些体会记录下。 什么是splunk简单来说，Splunk是一个托管的日志文件管理工具，它的主要功能包括：· 日志聚合功能· 搜索功能· 提取意义· 对结果进行分组，联合，拆分和格式化· 可视化功能· 电子邮件提醒功能而目前我通过splunk sdk主要实现的功能就是搜索功能，即搜索到我想要的日志。 Splunk SDK for Python通过Splunk SDK我们可以和Splunk引擎进行交互。Splunk SDK是基于REST API的，因此通过简短的代码实现我们想要的功能。官方文档：http://dev.splunk.com/python 安装！ 注意， splunk SDK 目前只支持python2，不支持python3.Ubuntu下安装步骤：1234567sudo apt-get install python-pip //第一步：安装pip工具pip freeze //查看是否已安装splunk sdkpip install splunk-sdk //第二步：下载splunk sdkexport PYTHONPATH=~/splunk-sdk-python //添加到python环境变量 模块介绍主要有4个模块： binding: 基于HTTP的抽象层 client: 基于REST API的抽象层，其中Service类是其最重要的类，并且client模块比binding模块有更多的好处。 results: 对splunk返回的数据进行处理 data: 将Atom Feed data转换为python格式目前我主要用到的模块是client和results模块。 与Splunk enterprise建立连接python21234567891011121314151617import splunklib.client as clientHOST = "localhost"PORT = 8089USERNAME = "admin"PASSWORD = "changeme"# Create a Service instance and log in service = client.connect( host=HOST, port=PORT, username=USERNAME, password=PASSWORD)# Print installed apps to the console to verify loginfor app in service.apps: print app.name 存在的问题：splunk服务器的8089没有打开，因此连接不上解决：splunk的服务器是在一个虚拟网络下的，8089端口不对公网开放，因此可以在该网络下另起一个ubuntu虚拟机，然后ubuntu虚拟机通过22端口与请求相连接，收到请求后，由于ubuntu和splunk server是在一个内网中，因此ubuntu虚拟机可以与splunk server的8089端口通信，获得数据后再返回给请求端。 Search的一些术语 search query:即查询命令的集合，例如：* | head 10```12345678910111213141516171819202122* saved search: 被保存的可以被再次使用的搜索查询。* search job：即一次搜索操作的具体实例。* Normal search: 异步搜索，能够立刻返回serach job而不用等到结果都搜索完。* Blocking search:同步搜索，只有所有搜索全都完成后才返回job.* Oneshot: blocking serach，直接返回搜索结果而不是返回job ...### 创建一个blocking serach与normal search的不同点是blocking search是堵塞的，也就是创建job后不会立刻返回，而是等待所有结果搜索结束后才返回，此时job包含了所有的搜索结果。```python# Get the collection of jobsjobs = service.jobs# Run a blocking search--search everything, return 1st 100 eventskwargs_blockingsearch = &#123;&quot;exec_mode&quot;: &quot;blocking&quot;&#125;searchquery_blocking = &quot;search * | head 100&quot;print &quot;Wait for the search to finish...&quot;# A blocking search returns the job&apos;s SID when the search is donejob = jobs.create(searchquery_blocking, **kwargs_blockingsearch)print &quot;...done!\n&quot; 创建一个normal searchnormal search 是异步的，即立刻返回job, 但是实际结果并没有返回，因此会存在操作job时没有event返回的情况，我目前的做法是，如果知道大致的搜索时间，则设置一定时间等待后再对job进行操作。123456789101112131415import splunklib.results as results# Initialize your service like so# import splunklib.client as client# service = client.connect(username="admin", password="changeme")searchquery_normal = "search * | head 10"kwargs_normalsearch = &#123;"exec_mode": "normal"&#125;job = service.jobs.create(searchquery_normal, **kwargs_normalsearch)# Get the results and display themfor result in results.ResultsReader(job.results()): print resultjob.cancel() 创建一个oneshot search1234567891011121314151617import splunklib.results as results# Run a one-shot search and display the results using the results reader# Set the parameters for the search:# - Search everything in a 24-hour time range starting June 19, 12:00pm# - Display the first 10 resultskwargs_oneshot = &#123;"earliest_time": "2014-06-19T12:00:00.000-07:00", "latest_time": "2014-06-20T12:00:00.000-07:00"&#125;searchquery_oneshot = "search * | head 10"oneshotsearch_results = service.jobs.oneshot(searchquery_oneshot, **kwargs_oneshot)# Get the results and display them using the ResultsReaderreader = results.ResultsReader(oneshotsearch_results)for item in reader: print(item) …待续]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>splunk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongo输入带'.'的字典]]></title>
    <url>%2F2018%2F09%2F12%2Fmongo%E8%BE%93%E5%85%A5%E5%B8%A6'.'%E7%9A%84%E5%AD%97%E5%85%B8%2F</url>
    <content type="text"><![CDATA[mongodb默认是不允许存储键中带’.’的字典，pymongo中实现的两种解决方案： 将字典键中的’.’用’_’等其他符号替代； 但是当字典包含多层嵌套字典，我觉得通过替换过于麻烦，所以我使用了另一种方案：collection.insert(docs, check_keys=False)即通过设置check_keys来使得mongo不检查’.’， 但是这个方案只限于临时解决方案，因为这个方案只适合于pymongo2.7版本，并且在后续版本中insert这个方法是被废弃的，而新增的insert_one和insert_many并没有checks_keys的属性。]]></content>
      <categories>
        <category>mongo</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>mongo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 正则表达式 匹配邮箱地址]]></title>
    <url>%2F2018%2F09%2F12%2FPython%20%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%20%E5%8C%B9%E9%85%8D%E9%82%AE%E7%AE%B1%E5%9C%B0%E5%9D%80%2F</url>
    <content type="text"><![CDATA[12345import repat = r'^(\w)+(\.\w+)*@(\w)+((\.\w+)+)$' email_address = 'ddy_davie@aaa.com'matched_address = re.match(pat, email_address)print(matched_address.group()) pat = r’^(\w)+(.\w+)*@(\w)+((.\w+)+)$’ ， 这个正则表达式目前尝试了基本可以覆盖大部分邮箱类型。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB 学习]]></title>
    <url>%2F2018%2F09%2F12%2FMongoDB%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[记录下我在学习mongoDB时遇到的一些问题与解决方案 问题：windows下启动mongoDB后显示等待连接端口，并且停止在这一步：123456782017-09-17T11:57:32.398+0800 I CONTROL [initandlisten]2017-09-17T11:57:32.398+0800 I CONTROL [initandlisten] ** WARNING: Access control is n2017-09-17T11:57:32.399+0800 I CONTROL [initandlisten] ** Read and write accenrestricted.2017-09-17T11:57:32.402+0800 I CONTROL [initandlisten]2017-09-17T11:57:32.966+0800 I FTDC [initandlisten] Initializing full-time diagnost:/data/db/diagnostic.data'2017-09-17T11:57:32.972+0800 I NETWORK [thread1] waiting for connections on port 27017 解决：其实，这不是卡住了，而是数据库已经启动，并且这个终端还不能关掉，关掉意味着数据库也关了。因此，保留这个终端，再开个终端，进入mongoDB\bin文件夹后输入.\mongo(注意，这里不是.\mongod)回车，就可以进入mongodb了 问题：ubuntu上出现无法连上mongodb的问题，显示已启用另一个终端：123mongod2017-09-20T04:07:41.308-0400 I STORAGE [initandlisten] exception in initAndListen: 98 Unable to create/open lock file: /data/db/mongod.lock errno:13 Permission denied Is a mongod instance already running?, terminating2017-09-20T04:07:41.308-0400 I CONTROL [initandlisten] dbexit: rc: 100 解决： 1sudo chown -R `id -u` /data/db 一句话解决，虽然我也不清楚原理。接下来同样是另起一个终端输入mongo即成功]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>mongo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-Valid Parentheses]]></title>
    <url>%2F2018%2F09%2F12%2FLeetcode-Valid%20Parentheses%2F</url>
    <content type="text"><![CDATA[Valid Parentheses验证有效括号对：给定一个只包含括号类型的字符串，判断该字符串的括号是否有效闭合。DescriptionGiven a string containing just the characters ‘(‘, ‘)’, ‘{‘, ‘}’, ‘[‘ and ‘]’, determine if the input string is valid.The brackets must close in the correct order, “()” and “()[]{}” are all valid but “(]” and “([)]” are not. 解题思路：采用栈来解决，依次检查给的characters，如果是左括号都入栈；如果是右括号，检查栈如果为空，证明不能匹配，如果栈不空，弹出top，与当前扫描的括号检查是否匹配。全部字符都检查完了以后，判断栈是否为空，空则正确都匹配，不空则证明有没匹配的。 代码中采用字典方式，将右括号作为键，将左括号作为值。123456789101112131415161718def isValid(self, s): """ :type s: str :rtype: bool """ stack = [] dict = &#123;')':'(', ']':'[', '&#125;':'&#123;'&#125; for char in s: if char in dict.values(): stack.append(char) elif char in dict.keys(): if stack==[] or stack.pop()!=dict[char]: return False else: return False return stack==[]]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-Valid Anagram]]></title>
    <url>%2F2018%2F09%2F12%2FLeetcode-Valid%20Anagram%2F</url>
    <content type="text"><![CDATA[Valid Anagram给定两个字符串，判断其中一个字符串是否是另一个字符串的异序组合。Given two strings s and t, write a function to determine if t is an anagram of s. For example,s = “anagram”, t = “nagaram”, return true.s = “rat”, t = “car”, return false. Description tip：anagram的意思是把单词的字母顺序打乱，重新排列后变成一个新单词。 解题思路1：将两个字符串排序后直接比较是否相同。 12345678class Solution(object): def isAnagram(self, s, t): """ :type s: str :type t: str :rtype: bool """ return sorted(s) == sorted(t) 解题思路2：采用字典的方式，分别记录两个字符串中每个字符的出现次数，比较字典是否相同。12345678910111213class Solution(object): def isAnagram(self, s, t): """ :type s: str :type t: str :rtype: bool """ dict1, dict2 = &#123;&#125;, &#123;&#125; for i in s: dict1[i] = dict1.get(i, 0) + 1 for j in t: dict2[j] = dict2.get(j, 0) + 1 return dict1 == dict2 tip:dict.get(key, default=None):对于键key返回其对应的值，或者若dict中不包含key则返回default(注意，default的默认值是None)。]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-Search Insert Position]]></title>
    <url>%2F2018%2F09%2F12%2FLeetcode-Search%20Insert%20Position%2F</url>
    <content type="text"><![CDATA[Search Insert Position给定一个有序的无重复元素的序列，和一个目标元素，求出该元素在数组中的下标，若数组中不存在该元素，则返回其在数组中的顺序下标。Description 解题思路：直接采用暴力查询。首先判断目标元素是否在数组中，若存在，则直接返回下标。若不存在，将目标元素与数组中的最大最小值比较；如果介于最大最小值之间，则与数组中的任意两个相邻元素进行比较。 1234567891011121314151617def searchInsert(self, nums, target): """ :type nums: List[int] :type target: int :rtype: int """ if target in nums: return nums.index(target) if target&lt;min(nums): return 0 elif target&gt;max(nums): return len(nums) else: for i in range(len(nums)): if nums[i]&lt;target and nums[i+1]&gt;target: return i+1 改进：采用二分查找的思路，leetcode上提供的另一种解法。 123456789101112131415161718192021222324252627def searchInsert(self, nums, key): if key &gt; nums[len(nums) - 1]: return len(nums) if key &lt; nums[0]: return 0 l, r = 0, len(nums) - 1 while l &lt;= r: m = (l + r)/2 if nums[m] &gt; key: r = m - 1 if r &gt;= 0: if nums[r] &lt; key: return r + 1 else: return 0 elif nums[m] &lt; key: l = m + 1 if l &lt; len(nums): if nums[l] &gt; key: return l else: return len(nums) else: return m]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-Roman to Integer]]></title>
    <url>%2F2018%2F09%2F12%2FLeetcode-Roman%20to%20Integer%2F</url>
    <content type="text"><![CDATA[Roman to Integer将罗马数字转换为阿拉伯数字。Description leetcode没有说明具体的转换规则，我是按照罗马数字规则设置的。 解题思路：一个保存罗马数字映射的字典:1roman = &#123;&apos;M&apos;: 1000,&apos;D&apos;: 500 ,&apos;C&apos;: 100,&apos;L&apos;: 50,&apos;X&apos;: 10,&apos;V&apos;: 5,&apos;I&apos;: 1&#125; 规则： 如果一个元素值比右边元素大，则加上这个元素； 如果一个元素值比右边元素小，则减去这个元素； 始终加上末尾的元素； 123456789101112131415def romanToInt(self, s): """ :type s: str :rtype: int """ roman = &#123;'M': 1000,'D': 500 ,'C': 100,'L': 50,'X': 10,'V': 5,'I': 1&#125; num = 0 for i in range(len(s)-1): if roman[s[i]]&gt;=roman[s[i+1]]: num = num + roman[s[i]] else: num = num - roman[s[i]] return num+roman[s[-1]]]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-Remove element]]></title>
    <url>%2F2018%2F09%2F12%2FLeetcode-Remove%20element%2F</url>
    <content type="text"><![CDATA[在我看来是remove duplicated elements from sorted array的姊妹题。 给定一个数组（未排序）和一个元素，要求从数组中去除这个元素并返回数组长度。超出数组长度的部分不在考虑范围内。Description 解题思路：利用两个指针，一个用来遍历整个数组，如果与给定的元素不相同，则利用另一个index下标将该元素加入到nums数组中。12345678910111213141516def removeElement(self, nums, val): """ :type nums: List[int] :type val: int :rtype: int """ if not nums: return 0 index = 0 for i in range(len(nums)): if nums[i] != val: nums[index] = nums[i] index += 1 return index 另外看见一个比较特别的解法：123456def removeElement(self, nums, val): try: while True: nums.remove(val) except: return len(nums)]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-Merge Two Sorted Lists]]></title>
    <url>%2F2018%2F09%2F12%2FLeetcode-Merge%20Two%20Sorted%20Lists%2F</url>
    <content type="text"><![CDATA[Merge Two Sorted Lists合并两个有序链表。Description 解题思路：此题比较简单，采用迭代的方式。由于链表已经是有序的，所以依次比较两个链表当前元素的大小，然后将较小的元素加入到新的链表中。 123456789101112131415161718192021222324# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def mergeTwoLists(self, l1, l2): """ :type l1: ListNode :type l2: ListNode :rtype: ListNode """ result = cur = ListNode(1) # 第一个节点可以任取 while l1 and l2: if l1.val &lt; l2.val: cur.next = l1 l1 = l1.next else: cur.next = l2 l2 = l2.next cur = cur.next cur.next = l1 or l2 return result.next]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-Remove Duplicates from Sorted List]]></title>
    <url>%2F2018%2F09%2F12%2FLeetcode-Remove%20Duplicates%20from%20Sorted%20List%2F</url>
    <content type="text"><![CDATA[Remove Duplicates from Sorted List从排序链表中删除重复元素。Description 解题思路：依次比较相邻的两个链表元素，若值相等，则将前一个节点的next引用为后一个节点的后一个节点。使用cur来依次向下遍历元素，最后返回head。 12345678910111213141516171819# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def deleteDuplicates(self, head): """ :type head: ListNode :rtype: ListNode """ cur = head while cur: while cur.next and cur.next.val==cur.val: cur.next = cur.next.next cur = cur.next return head]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-Remove Duplicates from Sorted Array]]></title>
    <url>%2F2018%2F09%2F12%2Fleetcode-Remove%20Duplicates%20from%20Sorted%20Array%2F</url>
    <content type="text"><![CDATA[给定一个排序数组，将数组中的重复元素去除，并返回修改后的数组长度。Description解题思路：题目要求不能分配额外空间，由于python中列表是传引用，因此可以对传入的列表进行原地修改。由于数组已经排序，因此可以通过新建一个下标两两比较删除重复的元素。注意：题目中 It doesn’t matter what you leave beyond the new length，因此超出长度部分的重复元素不需要考虑。 123456789101112131415def removeDuplicates(self, nums): """ :type nums: List[int] :rtype: int """ if not nums: return 0 newIndex = 0 for i in range(1, len(nums)): if nums[i] != nums[newIndex]: newIndex += 1 nums[newIndex] = nums[i] return newIndex+1]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-Intersection of Two Linked Lists]]></title>
    <url>%2F2018%2F09%2F12%2FLeetcode-Intersection%20of%20Two%20Linked%20Lists%2F</url>
    <content type="text"><![CDATA[Intersection of Two Linked Lists寻找两个无环链表的交点。Description 解题思路： 如果两个链长度相同的话，那么对应的一个个比下去就能找到； 如果两个链长度不相同，分别计算出两个链表的长度，计算出长度差值，然后让长度更长的那个链表从头节点先遍历长度差的步数，这样以后两个链表按尾部对齐。接着长链表和短链表同步往下走，遇到的第一个相同的节点就是最早的公共节点。 123456789101112131415161718192021222324252627282930# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def getIntersectionNode(self, headA, headB): """ :type head1, head1: ListNode :rtype: ListNode """ lenA = lenB = 0 curA, curB = headA, headB while curA is not None: lenA += 1 curA = curA.next while curB is not None: lenB += 1 curB = curB.next if lenA&gt;lenB: for i in range(lenA-lenB): headA = headA.next else: for i in range(lenB-lenA): headB = headB.next while headA != headB: headA = headA.next headB = headB.next return headA]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-Longest Common Prefix]]></title>
    <url>%2F2018%2F09%2F12%2FLeetcode-Longest%20Common%20Prefix%2F</url>
    <content type="text"><![CDATA[Longest Common Prefix最长相同前缀：给定一个字符串数组，找出其中最长的共同前缀。这里leetcode并没有说明共同前缀是指两两之间的前缀还是所有字符串的前缀，实际题意是指采用所有字符串的共同前缀。Description 解题思路：若字符串数组为空则返回空字符串；否则从所有字符串中找出最短的字符串，依次将最短字符串的每个元素和所有字符串对应位置上的元素进行比较，若不同则停止比较并返回；若全部相同，则返回最短字符串。 1234567891011if not strs: return ""shortest = min(strs, key=len)for i in range(len(shortest)): for string in strs: if shortest[i] != string[i]: return shortest[:i] return shortest 改进：将字符串数组进行排序(注意，这里是按照字母表顺序排序，而不是根据长度排序），排序后实际只要比较第一个和最后一个字符串的共同前缀即可，因此大大提升了运行时间。12345678910111213if not strs: return "" strs.sort() first = strs[0] last = strs[-1] minlen = min(len(first), len(last)) for i in range(minlen): if first[i]!=last[i]: return first[:i] return first[:minlen]]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-Maximum Subarray]]></title>
    <url>%2F2018%2F09%2F12%2FLeetcode-Maximum%20Subarray%2F</url>
    <content type="text"><![CDATA[Maximum Subarray最大子串和问题：从一个数组中找出一个子串使其和最大。Description tips:子串是指数组中连续的若干个元素，而子序列只要求各元素的顺序与其在数组中一致，而没有连续的要求。 解题思路：自己没有想出来，直接借鉴了网上的动态规划思路，我觉得下面是一些解决的关键点： 对于array[1…n]，如果array[i…j]就是满足和最大的子串，那么对于任何k(i&lt;=k&lt;=j)，我们有array[i…k]的和大于0。具体证明 假设已知0, .., k的最大和sum[k]以后，则0, …, k+1的最大和sum[k+1]分为以下两种情况：1）若sum[k]&gt;=0，则sum[k+1]=sum[k]+A[k+1]。代码中即curSum = curSum+nums[i]。2）若sum[k]&lt;0，另起一个SubArray，令sum[k+1]=A[k+1]。代码中即curSum = nums[i]。 1234567891011121314def maxSubArray(self, nums): """ :type nums: List[int] :rtype: int """ if not nums: return 0 curSum = maxSum = nums[0] for i in range(1, len(nums)): curSum = max(nums[i], curSum+nums[i]) maxSum = max(maxSum, curSum) return maxSum]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-Linked List Cycle]]></title>
    <url>%2F2018%2F09%2F12%2FLeetcode-Linked%20List%20Cycle%2F</url>
    <content type="text"><![CDATA[Linked List Cycle判断单链表是否存在环。Description 解题思路：一个单链表如果不存在环，则最后一个元素的下一个节点必然为null.如果单链表存在环，则：设置两个指针，一个慢指针和一个快指针。将链表的环想象成是一个圆形操场，两个人在同一起跑线开始绕着操场无限循环跑，那么快的人必然会再一次和慢的人相遇，即快指针的元素和慢指针的元素相同时，即说明存在环。在代码中，慢指针设置为每次走一步，即slow=slow.next,快指针设置为每次走两步，即fast=fast.next.next。实际步数设置是可以任取的，只是循环次数不同，如可以设置fast=fast.next.next.next。 1234567891011121314151617181920212223# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def hasCycle(self, head): """ :type head: ListNode :rtype: bool """ if head==None: return False slow = head fast = head while fast.next and fast.next.next: slow = slow.next fast = fast.next.next if slow==fast: return True return False 改进：由于每次循环都要判断是否到达最后一个节点，因此leetcode上提供了另一种思路，即采取异常的方式。 “Easier to ask for forgiveness than permission.”12345678910def hasCycle(self, head): try: slow = head fast = head.next while slow is not fast: slow = slow.next fast = fast.next.next return True except: return False]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-Intersection of Two Arrays]]></title>
    <url>%2F2018%2F09%2F12%2FLeetcode-Intersection%20of%20Two%20Arrays%2F</url>
    <content type="text"><![CDATA[Intersection of Two Arrays查找两个数组的共有元素。Given two arrays, write a function to compute their intersection.Example:Given nums1 = [1, 2, 2, 1], nums2 = [2, 2], return [2].Description 解题思路：本来是想做关于排序的题目的，然后这道题的标签中就是含sort的，但我觉得用集合的方式会更简明些。 12345678910class Solution(object): def intersection(self, nums1, nums2): """ :type nums1: List[int] :type nums2: List[int] :rtype: List[int] """ set1 = set(nums1) set2 = set(nums2) return list(set1.intersection(set2)) tip：set求交集函数返回的是set,因此需要通过list进行转换。]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-Implement strStr()]]></title>
    <url>%2F2018%2F09%2F12%2FLeetcode-Implement%20strStr()%2F</url>
    <content type="text"><![CDATA[Implement strStr()从字符串中找出给定子字符串的索引，若不存在则返回-1。Description 解题思路：用python解决这道题很简单，因为python字符串自带的find的方法可以直接实现。1234567def strStr(self, haystack, needle): """ :type haystack: str :type needle: str :rtype: int """ return haystack.find(needle) 不采用find()方法的解题思路：采用brute force方式，即依次从字符串的每个位置开始，截取和子字符串相同长度的字符串，与给定的子字符串进行比较。12345678910def strStr(self, haystack, needle): """ :type haystack: str :type needle: str :rtype: int """ for i in range(len(haystack)-len(needle)+1): if haystack[i:i+len(needle)] == needle: return i return -1 改进：http://blog.csdn.net/linhuanmars/article/details/20276833提出了一种rolling hash的方式。]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-House Robber]]></title>
    <url>%2F2018%2F09%2F12%2FLeetcode-House%20Robber%2F</url>
    <content type="text"><![CDATA[House Robber打家劫舍。问题本质就是从数组中找出一个或多个不相邻的数，使其和最大。 You are a professional robber planning to rob houses along a street. Each house has a certain amount of money stashed, the only constraint stopping you from robbing each of them is that adjacent houses have security system connected and it will automatically contact the police if two adjacent houses were broken into on the same night. Given a list of non-negative integers representing the amount of money of each house, determine the maximum amount of money you can rob tonight without alerting the police. Description 解题思路： 如果选择了抢劫上一个屋子，那么就不能抢劫当前的屋子，所以最大收益就是抢劫上一个屋子的收益； 如果选择抢劫当前屋子，就不能抢劫上一个屋子，所以最大收益是到上一个屋子的上一个屋子为止的最大收益，加上当前屋子里有的钱。 令dp[i]表示从[0, …, i]数组中能够得到的最大收益，显然，dp[0]=nums[0], dp[1]=max(dp[0],dp[1]), 现在计算dp[i+1]: dp[i+1]=max(dp[i], dp[i-1]+nums[i+1]) 其中，上式包含了如下隐含信息：若dp[i]不包含nums[i], 所以nums[i+1]可以和dp[i]相加，但此时dp[i]=dp[i-1],所以dp[i-1]+nums[i+1]与dp[i]+nums[i+1]的值相等；若dp[i]包含nums[i]，则nums[i+1]不可以和dp[i]相加，所以只存在nums[i+1]+dp[i-1]。 实际计算时只需要保存两个变量即可。 1234567891011121314151617class Solution(object): def rob(self, nums): """ :type nums: List[int] :rtype: int """ if len(nums)&lt;=1: return 0 if len(nums)==0 else nums[0] # 保存上一次的收益 last = nums[0] # 保存当前收益 cur = max(nums[0], nums[1]) for i in range(2, len(nums)) : tmp = cur cur = max(last+nums[i], cur) last = tmp return cur tip:python中的三目运算符不像其他语言, 其他的一般都是: 判定条件?为真时的结果:为假时的结果 而在python中的格式为: 为真时的结果 if 判定条件 else 为假时的结果]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-Count and Say]]></title>
    <url>%2F2018%2F09%2F12%2FLeetcode-Count%20and%20Say%2F</url>
    <content type="text"><![CDATA[Count and Sayleetcode关于这题的说明比较含糊：The count-and-say sequence is the sequence of integers with the first five terms as following: 1 11 21 1211 111221 1 is read off as “one 1” or 11.11 is read off as “two 1s” or 21.21 is read off as “one 2, then one 1” or 1211.Given an integer n, generate the nth term of the count-and-say sequence. 题意是n=1时输出字符串1；n=2时，数上次字符串中的数值个数，因为上次字符串有1个1，所以输出11；n=3时，由于上次字符是11，有2个1，所以输出21；n=4时，由于上次字符串是21，有1个2和1个1，所以输出1211。依次类推，写个countAndSay(n)函数返回字符串。 解题思路：采用动态规划的方式。当n=1时，直接返回‘1’。自底向上，然后根据n=index时返回的字符串计算出n=index+1时的字符串，并保存到数组curString中。1234567891011121314151617181920def countAndSay(self, n): if n==1: return '1' count = 1 index = 1 curString = ['', '1'] while index != n: result = '' cs = curString[index] + '*' # 加*是为了下面计算方便 for i in range(len(cs)-1): if cs[i]==cs[i+1]: count += 1 else: result += str(count) + cs[i] # 加*的目的是为了方便处理最后一个字符 count = 1 index += 1 curString.append(result) return curString[n] 采用正则表达：leetcode上提供的另一种解题思路，尽管运行比较慢。用作者的解释即: (.) captures one character and \1* covers its repetitions.其中\1代表后向引用，表示表达式中，从左往右数，第一个左括号对应的括号内的内容。以此类推，\2表示第二个，\0表示整个表达式。12345def countAndSay(self, n): s = '1' for _ in range(n - 1): s = re.sub(r'(.)\1*', lambda m: str(len(m.group(0))) + m.group(1), s) return s]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-Best Time to Buy and Sell Stock]]></title>
    <url>%2F2018%2F09%2F12%2FLeetcode-Best%20Time%20to%20Buy%20and%20Sell%20Stock%2F</url>
    <content type="text"><![CDATA[Best Time to Buy and Sell Stock买卖股票的最佳时机，返回最大收益。Say you have an array for which the ith element is the price of a given stock on day i. If you were only permitted to complete at most one transaction (ie, buy one and sell one share of the stock), design an algorithm to find the maximum profit.Description 解题思路：依次遍历整个数组，利用minprice这个变量来保存每个元素之前的最低价格，并计算出在该元素卖出时能够带来的最大收益，最后将每个元素卖出时得到的最大收益进行比较即得到整个系统的最大收益。 12345678910111213class Solution(object): def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ maxprofit = 0 minprice = float('inf') for price in prices: minprice = min(minprice, price) profit = price - minprice maxprofit = max(maxprofit, profit) return maxprofit tip:Python中可以用如下方式表示正负无穷：float(“inf”), float(“-inf”)]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-Climbing Stairs]]></title>
    <url>%2F2018%2F09%2F12%2FLeetcode-Climbing%20Stairs%2F</url>
    <content type="text"><![CDATA[Climbing Stairs爬梯子问题。给定一个n级台阶，每次可以走一个台阶或者两个台阶，一共有多少种走法？Description 解题思路：很常见的一种递推题型，要求n级台阶的走法，即可以分解为求n-1级台阶加上n-2级台阶的走法，climbNum[n]=climbNum[n-1]+climbNum[n-2]。所以问题实质上就是求解斐波那契数列。但由于采用递推方式会产生大量重复的计算，因此使用动态规划自底向上的进行计算，其中我们使用一个数组用于保存每步产生的结果。 123456789101112class Solution(object): def climbStairs(self, n): """ :type n: int :rtype: int """ if n&lt;=2: return n climbNum = [1,1,2] for i in range(3,n+1): climbNum.append(climbNum[i-1] + climbNum[i-2]) return climbNum[n]]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-2sum-python]]></title>
    <url>%2F2018%2F09%2F12%2Fleetcode-2sum%2F</url>
    <content type="text"><![CDATA[Given nums = [2, 7, 11, 15], target = 9, Because nums[0] + nums[1] = 2 + 7 = 9, return [0, 1].Description 1234567891011class Solution: def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ for i in range(0, len(nums)): for j in range(i+1, len(nums)): if nums[i]+nums[j]==target: return [i, j] 初始版本的思路：依次将两个数相加，由于题目中说明只存在一对答案，因此如果存在符合条件的两个下标，就是需要的答案。测试未通过：leetcode要求需要满足时间复杂度。 12345678910class Solution(object): def twoSum(self, nums, target): if len(nums) &lt;= 1: return False buff_dict = &#123;&#125; for i in range(len(nums)): if nums[i] in buff_dict: return [buff_dict[nums[i]], i] else: buff_dict[target - nums[i]] = i leetcode上提供的一种O(n)的python解法，使用字典, python字典采用hash方式。解题思路：建立一个字典，依次比较列表中的元素。如果字典的键中不存在该元素，就将target - nums[i]作为字典的键，并将对应的下标作为值；如果字典键中存在该元素，则说明字典中存在符合要求的对应元素，通过buff_dict[nums[i]]提取出另一个元素的下标。]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Batch Normalization 批标准化]]></title>
    <url>%2F2018%2F09%2F12%2FBatch%20Normalization%2F</url>
    <content type="text"><![CDATA[批标准化，相当于深度学习中的数据归一化。 普通机器学习的归一化只针对输入层，而BN对中间每一层输入（激活函数处理前的数值，我称之为net值）进行归一化。 归一化的目的是使的数据的分布集中在激活函数的敏感区域（即中间线性区域）。 BN层就是对深度学习的中间层的输入数据（net值）进行归一化，解决在训练过程中，中间层数据分布发生改变的情况，使得每一层的分布相似(标准正态)，同时引入非线性, 保留原来每层训练得到的特征分布（通过变换重构）。 机器学习领域的重要假设：独立同分布假设，即假设训练集数据和测试集数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。 BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。 Mini-batch 相对于 SGD的两个优点：梯度更新方向更准确；并行计算速度快。 BatchNorm就是基于mini-batch的，这也是名称中Batch的由来。 随机梯度下降成了训练深度网络的主流方法。尽管随机梯度下降法对于训练深度网络简单高效，但是它有个毛病，就是需要我们人为的去选择参数，比如学习率、参数初始化、权重衰减系数、Drop out比例等。这些参数的选择对训练结果至关重要，以至于我们很多时间都浪费在这些的调参上。 采用Batch Normalization的好处提升了训练速度，收敛过程大大加快。 (1)你可以选择比较大的初始学习率，让你的训练速度飙涨。以前还需要慢慢调整学习率，甚至在网络训练到一半的时候，还需要想着学习率进一步调小的比例选择多少比较合适，现在我们可以采用初始很大的学习率，然后学习率的衰减速度也很大，因为这个算法收敛很快。当然这个算法即使你选择了较小的学习率，也比以前的收敛速度快，因为它具有快速训练收敛的特性； (2)你再也不用去理会过拟合中drop out、L2正则项参数的选择问题，采用BN算法后，你可以移除这两项了参数，或者可以选择更小的L2正则约束参数了，因为BN具有提高网络泛化能力的特性，避因为这是类似于dropout的一种防止过拟合的正则化方式； 原因：均值和方差是在mini-batch上计算的，而不是在整个训练集，因此均值和方差会存在噪声，所以和dropout类似，它往每个隐藏层的激活值上增加了噪音：dropout增加噪音的方式，使一个隐藏的单元，以一定的概率乘以0. 但因为添加噪音较小，因此正则化效果轻微，可以和dropout一起使用。 (3)再也不需要使用局部响应归一化层了（局部响应归一化是Alexnet网络用到的方法，搞视觉的估计比较熟悉），因为BN本身就是一个归一化网络层； (4)可以把训练数据彻底打乱（防止每批训练的时候，某一个样本都经常被挑选到）。 深度神经网络中的归一化问题 Internal Covariate Shift：原因在于神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。 我们知道网络一旦train起来，那么参数就要发生更新，除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。以网络第二层为例：网络的第二层输入，是由第一层的参数和input计算得到的，而第一层的参数在整个训练过程中一直在变化，因此必然会引起后面每一层输入数据分布的改变。我们把网络中间层在训练过程中，数据分布的改变称之为：“Internal Covariate Shift”。 BN概述基本思想：深层神经网络在做非线性变换前的激活输入值net值随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往激活函数的取值区间的上下限两端靠近，导致反向传播时底层神经网络的梯度消失，这就是深层神经网络收敛越来越慢的原因。 BN就是通过规范化手段，把每层神经网络任意神经元输入值net值的分布强行拉回到均值为0方差为1的标准正态分布，使得激活输入值落在中间比较敏感的区域的概率较大，让梯度变大，避免梯度消失问题，梯度变大意味着学习收敛速度快，加快训练速度。 如图，转换为标准正态分布后，数据落在-4，4的概率变大，而在这个区间范围内，对应的激活函数的梯度较大。 但是：由于数据都落在了激活函数的线性区域，相当于做了线性变换，那么整个网络就会变成多层线性网络，表达能力下降。为了解决这个问题，BN为了保证非线性的获得，对变换后满足均值为0方差为1的x又进行了scale加上shift操作（y=sclae*x + shift)，而这两个参数是通过训练学习得到的，意思是通过scale和shift把这个值从标准正态分布进行一些移动和变形，使得数据的散落区域从正中心向非线性区间进行部分移动。 训练 就像激活函数层、卷积层、全连接层、池化层一样，BN(Batch Normalization)也属于网络的一层。 BN层是对每个隐层的net值进行变换。经过BN变换以后的值再进过激活函数得到out值。 对于mini-batch来说，一次训练包含m个训练样本，因此对于隐层的每个神经元的激活值来说包括： 预处理： 注意：这里t层第k个神经元的并不是指原始输入，而是该神经元隐层的net值的第k维数据。 变换的意思是：某个神经元对应的net值减去mini-batch内m个样本在该层对应维度神经元的m个net值的均值并除以方差。也就是说，这个变换和标准归一化一样，对每个样本的每个维度的归一化需要所有样本该维度的数据。 某个神经元的net值在经过变换后就会形成服从均值为0，方差为1的正态分布。 为了避免数据始终落在激活函数的线性区域，导致网络的表达能力下降，因此引入两个调节参数scale和shift,对变换后的net值进行变换的反操作 引入了两个可学习参数γ和β： 每一个神经元$x^k$都会有一对这样的参数γ、β。这样其实当： 是可以恢复出原始的某一层所学到的特征的。因此我们引入了这个可学习重构参数γ、β，让我们的网络可以学习恢复出原始网络所要学习的特征分布。最后Batch Normalization网络层的前向传导过程公式就是： 上面的公式中m指的是mini-batch size。 预测阶段训练阶段可以根据mini-batch的若干个样本进行变换。 但是测试阶段我们一般只输入一个测试样本。我们可以采用所有训练实例中获得的统计量来代替mini-batch中m个训练实例的均值和方差统计量。 计算全局统计量时只需利用每次mini-batch计算的均值和方差统计量，不需要重新计算, 这里方差采用的是无偏方差估计。 有了均值和方差，并且每个隐层scaling和shift参数已经计算好，在预测时就可以采用： 这个公式其实和： 等价,就是代入而已，不过实际计算时可以减少计算量。 参考： BN详细笔记：https://www.cnblogs.com/guoyaohua/p/8724433.html Batch normalization 学习笔记：https://blog.csdn.net/hjimce/article/details/50866313 批标准化过程：https://blog.csdn.net/whitesilence/article/details/75667002]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>正则化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django搭建个人博客]]></title>
    <url>%2F2018%2F09%2F12%2FDjango%20blog%2F</url>
    <content type="text"><![CDATA[最近学着用Django搭建了个个人博客，目前域名正在备案中，现在把我这最近半个月的摸爬滚打经历记录下。 Django入门 Python核心编程 追梦人物的 Django博客教程 菜鸟Django教程 自强学堂Django教程 第一阶段：我首先看了Python核心编程中的Django章节，刚开始看的有些云里雾里。于是我去菜鸟教程那里按着Django教程的步骤搭建了下，菜鸟教程Django部分比较简单，之后对整体有了大致了解后再进一步看核心编程，发现能够比较容易理解了。 第二阶段：这里非常推荐追梦人物博主的博客教程，写的非常详细，按着博主的步骤就能够一步一步的把个人博客搭建起来，并且博主也提到了不少学习资料，对我们这些初学者能提供极大的帮助。另外自强学堂的内容我大致看了下，写的比较细，把个人博客搭建起来后再看可能效果会更好。 Django博客搭建我这里就放一些我在搭建博客时用到的一些不错的资源。 Bootstrap模板：https://html5up.net/这个网站提供了免费的响应式模板，我觉得很漂亮。 日历插件：http://www.jq22.com/yanshi11367这是我使用的日历插件，也可以在这个网站上找到一些别的datepicker的日历插件。 abowman:http://abowman.com/这个网站很有趣，可以在网页上显示一个有趣的动图，有需要的可以自己插入网页。 域名和云服务器我这里是在阿里云上学生优惠买的云服务器，域名也是直接在阿里云购买的。域名购买后记得要进行解析，还有备案（备案真的好麻烦）。 使用nginx后无法出现欢迎界面这个地方我摸索了好久，一直不知道怎么回事，在阿里云服务器上输入sudo service nginx start后，通过域名访问始终无法显示nginx的欢迎界面。后来发现原来是阿里云服务器的安全组设置中没有配置80端口。解决：进入阿里云ECS控制台-&gt;安全组-&gt;配置规则-&gt;公网入方向-&gt;快速配置规则-&gt;选中80端口和443端口，授权对象为0.0.0.0/0，确定即可。 在进展到用nginx和gunicorn步骤时始终出现欢迎界面。解决：进入/etc/nginx/sites-enabled删除default文件，原因是默认配置覆盖了我们的配置。 目前我还在慢慢学习中，待续… – 2018-09-12更新之前的django博客确实搭建成了，当时主要是为了成为一名网络开发工程师，摸索着完成了这一系列步骤。不过，现在转行成了一名算法工程师，也就没有心思继续维护之前的网站了。所以，为了简单，我采用了hexo+next+github的方式重新建立了新的博客，欢迎大家访问我的新的博客：hellodavid.top]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Django</tag>
        <tag>python</tag>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性回归和逻辑回归的比较]]></title>
    <url>%2F2018%2F09%2F11%2Flinear%20and%20logistic%20learning%2F</url>
    <content type="text"><![CDATA[线性回归 用一组变量的（特征）的线性组合，来建立与结果之间的关系。 模型表达：$y(x, w)=w_0+w_1x_1+…+w_nx_n$ 逻辑回归逻辑回归用于分类，而不是回归。 在线性回归模型中，输出一般是连续的， 对于每一个输入的x，都有一个对应的输出y。因此模型的定义域和值域都可以是无穷。 但是对于逻辑回归，输入可以是连续的[-∞, +∞]，但输出一般是离散的，通常只有两个值{0, 1}。 这两个值可以表示对样本的某种分类，高/低、患病/ 健康、阴性/阳性等，这就是最常见的二分类逻辑回归。因此，从整体上来说，通过逻辑回归模型，我们将在整个实数范围上的x映射到了有限个点上，这样就实现了对x的分类。因为每次拿过来一个x，经过逻辑回归分析，就可以将它归入某一类y中。 逻辑回归与线性回归的关系可以认为逻辑回归的输入是线性回归的输出，将逻辑斯蒂函数（Sigmoid曲线）作用于线性回归的输出得到输出结果。 线性回归y = ax + b, 其中a和b是待求参数； 逻辑回归p = S(ax + b), 其中a和b是待求参数， S是逻辑斯蒂函数，然后根据p与1-p的大小确定输出的值，通常阈值取0.5，若p大于0.5则归为1这类。 具体的： 线性函数如下： 构造预测函数： 逻辑回归的损失函数： 逻辑回归采用交叉熵作为代价函数，即对数损失函数。能够有效避免梯度消失. 对数损失函数（logarithmic loss function) 或对数似然损失函数(log-likehood loss function)： $$L(Y,P(Y|X))=−logP(Y|X)$$ 逻辑回归中，采用的是负对数损失函数。如果损失函数越小，表示模型越好。 极大似然估计： 极大似然原理的直观想法是，一个随机试验如有若干个可能的结果A，B，C，… ，若在一次试验中，结果A出现了，那么可以认为实验条件对A的出现有利，也即出现的概率P(A)较大。一般说来，事件A发生的概率与某一未知参数θ有关， θ取值不同，则事件A发生的概率也不同，当我们在一次试验中事件A发生了，则认为此时的θ值应是一切可能取值中使P(A|θ）达到最大的那一个，极大似然估计法就是要选取这样的θ值作为参数的估计值，使所选取的样本在被选的总体中出现的可能性为最大。 在逻辑回归中目标函数均为最大化条件概率p(y|x),其中x是输入样本 似然：选择参数使似然概率p(y|x,)最大，y是实际标签 过程：目标函数→单个样本的似然概率→所有样本的似然概率→log变换, 将累乘变成累加→负号, 变成损失函数 选择一组参数使得实验结果具有最大概率。 损失函数的由来： 已知估计函数为： 则似然概率分布为（即输出值为判断为1的概率，但在输出标签值时实际只与0.5作比较）： 可以写成概率一般式： 由最大似然估计原理，我们可以通过m个训练样本值，来估计出值，使得似然函数值（所有样本的似然函数之积）最大 求log: 取负数，得损失函数： 逻辑回归参数迭代，利用反向传播进行计算： 上面这个过程计算的是单个样本对wj的梯度更新。 为什么逻辑回归采用似然函数，而不是平方损失函数？可以从两个角度理解。 交叉熵损失函数的好处是可以克服方差代价函数更新权重过慢的问题（针对激活函数是sigmoid的情况）。 原因是其梯度里面不在包含对sigmoid函数的导数： 而如果使用的是平方损失函数加sigmoid函数，则计算梯度时： 会包含sigmoid的导数(sigmoid的导数值始终小于1），使梯度下降变慢。 图1 最小二乘作为逻辑回归模型的损失函数(非凸），theta为待优化参数 ​ 图2 最大似然作为逻辑回归模型的损失函数，theta为待优化参数 逻辑回归为什么使用sigmoid函数也可以从两点来进行理解 Sigmoid 函数自身的性质 因为这是一个最简单的，可导的，0-1阶跃函数 sigmoid 函数连续，单调递增 sigmiod 函数关于（0，0.5） 中心对称 对sigmoid函数求导简单 逻辑回归函数的定义 因此, 逻辑回归返回的概率是指判别为1类的概率. 逻辑回归和SVM的异同点相同点： 第一，LR和SVM都是分类算法。 第二，如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性的。 第三，LR和SVM都是监督学习算法。 第四，LR和SVM都是判别模型。 判别模型会生成一个表示P(Y|X)的判别函数（或预测模型），而生成模型先计算联合概率p(Y,X)然后通过贝叶斯公式转化为条件概率。简单来说，在计算判别模型时，不会计算联合概率，而在计算生成模型时，必须先计算联合概率。 不同点： 第一，本质上是其loss function不同 逻辑回归的损失函数是交叉熵函数： SVM的损失函数： 逻辑回归方法基于概率理论，假设样本为1的概率可以用sigmoid函数来表示，然后通过极大似然估计的方法估计出参数的值； 支持向量机基于几何间隔最大化原理，认为存在最大几何间隔的分类面为最优分类面； 第二，支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用）。 第三，在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。 这个问题理解起来非常简单。分类模型的结果就是计算决策面，模型训练的过程就是决策面的计算过程。通过上面的第二点不同点可以了解，在计算决策面时，SVM算法里只有少数几个代表支持向量的样本参与了计算，也就是只有少数几个样本需要参与核计算（即kernal machine解的系数是稀疏的）。然而，LR算法里，每个样本点都必须参与决策面的计算过程，也就是说，假设我们在LR里也运用核函数的原理，那么每个样本点都必须参与核计算，这带来的计算复杂度是相当高的。所以，在具体应用时，LR很少运用核函数机制。 第四，SVM的损失函数就自带正则 参考： Poll的博客：www.cnblogs.com/maybe2030/p/5494931.html 逻辑回归：https://www.cnblogs.com/Belter/p/6128644.html 逻辑回归推导：http://blog.csdn.net/pakko/article/details/37878837 极大似然估计：http://blog.csdn.net/star_liux/article/details/39666737 刘建平：https://www.cnblogs.com/pinard/p/6035872.html 逻辑回归和SVM的比较：https://www.cnblogs.com/zhizhan/p/5038747.html]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L1和L2正则化]]></title>
    <url>%2F2018%2F09%2F11%2FL1%20and%20L2%2F</url>
    <content type="text"><![CDATA[L0范数，L1范数，L2范数L0范数是指向量中非0元素的个数。 如果我们用L0范数来规则化一个参数矩阵W的话（正则项），就是希望W的大部分元素都是0。换句话说，让参数W是稀疏的。 L1范数是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。 为什么要稀疏？特征选择(Feature Selection)： 稀疏规则化它能实现特征的自动选择。一般来说，一个样本的大部分特征都是和最终的输出没有关系或者不提供任何信息的，在最小化目标函数的时候考虑这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息被考虑反而干扰了对正确标签的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。 可解释性(Interpretability)： 另一个青睐于稀疏的理由是，模型更容易解释。例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。假设我们这个是个回归模型：$y=w_1x_1+w_2x_2+…+w_{1000}x_{1000}+b$ (为了让y限定在[0,1]的范围，一般还得加个Logistic函数）。通过学习，如果最后学习到的w就只有很少的非零元素，例如只有5个非零的$w_i$，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个$w_i$都非0，医生面对这1000种因素，累觉不爱。 L2范数是指向量各元素的平方和然后求平方根。作用是改善过拟合。 在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减weight decay”。 实际上，对于L1和L2规则化的代价函数来说，我们可以写成以下形式： 为什么L1正则化可以取稀疏解等值线图的理解： 目标函数由两部分组成：原始的损失函数和正则项 其中$J_0$是原始的损失函数，是关于权重w的函数，上图中的等值线图考虑了只有两个权重系数的情况。 右上方的彩色圆圈是关于原始损失函数的等值线图，可以想象成是一个曲面压平后的结果。 因此，每一圈上的每个坐标带入原始损失函数得到的损失值是相同的。 由于原始损失函数是凸函数，因此紫色中心是取值最小的，也就是说，不加正则项，我们应该取紫色点的权重坐标。而越往外扩散，原始损失函数的取值越大。 注意：等值线图并没有画全，还可以继续向外扩散。 正则项相当于对原始损失函数做了约束，令L代表正则项，因此黑色线圈表示满足约束的权重坐标。 如果是L1， 则$L=|w_1| + |w_2|$， 若为L2， 则$L = |w_1|^2 + |w_2|^2$ 因此，为了要满足约束，我们要取黑色线与彩色线的交点，但是同时为了使彩色线的取值尽可能小，我们需要取尽可能靠近紫色线方向的线圈，因此最终取图中的交点。否则，如果和黑色线上别的交点相交，彩色线圈会往外移，导致取值增加。 为什么L2可以获得平滑的解？(从理论角度)假设原始损失函数为： 此时，进行梯度迭代的公式为： 如果加上L2正则项，则梯度更新公式变为： 因此，与未添加L2的情况相比，参数每次迭代都要乘以一个小于1的因子，从而使得参数不断减小。 为什么正则化/参数较小可以防止过拟合？ 直观理解就是：正则项λ越大，那么权重就会越小，使得神经网络变得简单。 根据激活函数理解，假设激活函数是tanh: 可以发现，当z值较小时，激活函数的部分几乎为线性形状。 如果正则化项λ很大，那么权重就会很小，因此相对来说，z=wx+b也会很小，因此在这段范围内，激活函数呈线性，和线性回归类似，因此，当每层网络都是线性网络，那么整个网络都是一个线性网络，无法适用于复杂决策，不会发生过拟合。 过拟合的时候，拟合函数的系数往往非常大，为什么？如下图所示，过拟合，就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。 而正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。 贝叶斯派解释： 加2范数正则等价于加了高斯分布的先验，加1范数正则相当于加拉普拉斯分布先验。 为什么L2得到的系数趋于相同L2正则化将系数向量的L2范数添加到了损失函数中。由于L2惩罚项中系数是二次方的，这使得L2和L1有着诸多差异，最明显的一点就是，L2正则化会让系数的取值变得平均。对于关联特征，这意味着他们能够获得更相近的对应系数。还是以Y=X1+X2为例，假设X1和X2具有很强的关联，如果用L1正则化，不论学到的模型是Y=X1+X2还是Y=2X1，惩罚都是一样的，都是2alpha。但是对于L2来说，第一个模型的惩罚项是2alpha，但第二个模型的是4*alpha。可以看出，系数之和为常数时，各系数相等时惩罚是最小的，所以才有了L2会让各个系数趋于相同的特点。 总结因此，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。 L1正则假设参数的先验分布是Laplace分布，可以保证模型的稀疏性，也就是某些参数等于0； L2正则假设参数的先验分布是Gaussian分布，可以保证模型的稳定性，也就是参数的值不会太大或太小； 在实际使用中，如果特征是高维稀疏的，则使用L1正则；如果特征是低维稠密的，则使用L2正则。 参考： 百度知道：https://zhidao.baidu.com/question/1864240911512690707.html 博客：http://blog.csdn.net/zouxy09/article/details/24971995 L2正则化导致参数较小：https://blog.csdn.net/jinping_shi/article/details/52433975 等值线图形的解释：https://blog.csdn.net/weixin_39845112/article/details/80114918]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>正则化</tag>
        <tag>机器学习</tag>
        <tag>过拟合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[因子分解机 FM和FFM]]></title>
    <url>%2F2018%2F09%2F11%2FFM%20%26%20FFM%2F</url>
    <content type="text"><![CDATA[因子分解机 Factorization Machine因子分解机主要是考虑了特征之间的关联。 FM主要是为了解决数据稀疏的情况下，（而SVM无法解决稀疏问题），特征怎样组合的问题。 数据稀疏是指数据的维度很大，但是其中为0的维度很多。推荐系统是常见应用场景，原因是推荐系统中类别属性（如商品id）比较多，每一种类别属性经过onehot处理后会产生大量值为0的特征，导致样本变得稀疏，而FM就可以解决这种样本稀疏的问题。 因子分解机FM算法可以处理如下三类问题： 普通线性模型我们将各个特征独立考虑，并没有考虑特征之间的相互关系。 FM模型为了表述特征间的相关性，我们采用多项式模型，将特征$x_i$和$x_j$的组合用$x_ix_j$表示，只讨论二阶多项式模型： 其中，n表示样本的维度（是已经进行onehot以后的特征数量），$x_i$表示第i个特征，(如果是类别变量，那么onehot后只有一个维度值为1，其余维度值为0，因此在这种情况下$x_i$的值通常是取0和1，而对于一般的数值维度，$x_i$的值对应原来的数值)， $w_{ij}$是组合参数，代表组合特征的重要性，注意：$w_{ij}$和$w_{ji}$是相等的，因此组合特征部分相关参数共有$(n-1)+(n-2)+…+1=n(n-1)/2$ 注意到，在数据稀疏的情况下，满足特征$x_i$和$x_j$都不为0的情况很少，因此$w_{ij}$很难训练。 为了求解组合参数$w_{ij}$, 对每个特征分量$x_i$引入k维（k远小于n) 的辅助向量$v_i=(v_{i1}, v_{i2},…,v_{ik})$, 然后利用向量内积的结果$v_iv_j^T$来表示原来的组合参数$w_{ij}$ 实际上，辅助向量可以理解为是特征分量的另一种表示形式，类似于词向量的表示形式，但是和词向量存在区别。词向量中是将一个单词转换为向量表示形式，而单词是固定的，因此一个单词对应一个词向量；而在FM中，我们是将一个类别特征（注意，这是onehot前的特征）转换为一个向量，但由于该类别特征可以有多种取值，并且每种取值对应一个向量(也就是上面将类别特征onehot以后，每个特征分量对应一个辅助向量），因此，FM中确实是将一个类别特征转换为了向量形式，只不过向量会根据特征的取值发生变化。 此时，组合参数$w_{ij}$组成的矩阵可以表示为： 将组合参数进行分解的好处： 从原来要求n(n-1)/2个组合参数变成了求矩阵V，参数数量变为n*k. 削弱了高阶参数间的独立性：k越大（即对特征分量的表征能力越强），高阶参数间独立性越强，模型越精细；k越小，泛化能力越强， 因此实际问题选择较小的k可以克服稀疏数据的问题，并获得较好的预测效果。 因此时间复杂度从O(n^2)变成了O(kn) 此时，分解机的表示形式变为： 注意第二项，下标j的循环从i+1开始。 使用辅助向量乘积表示组合参数的原理： 通常，由于数据稀疏，本来组合参数是学习不到的，但是我们可以通过特征i与其他特征的数据的关系，特征j和其他特征的关系，分别学习到特征i和特征j的对应的辅助向量$v_i$和$v_j$,这样利用$v_iv_j^T$来表示$w_{ij}$，便可以解决数据稀疏带来的问题。 计算模型的预测值： 在计算模型时，只需要考虑计算量最大的二次项： 可以先把标量$x_i$和对应的辅助向量$v_i$相乘，并记录下来，得到$u_i=x_iv_i$, 注意$x_i$只是标量。 对于n个元素，共需要n*k次乘法，于是二元项变为：注意：公式中的r即为k,即辅助向量的维度 把上式凑成和的平方： 化简的原理是将整个对称矩阵W除去对角线上的数值，由于对称，再除以2得到原来的上三角矩阵。 括号内，两部分计算量均为O(n),因此整体计算量为O(kn)。 梯度下降求解模型参数： SGD中，需要计算两种导数： 预测值对一元参数的导数： 预测值对二元参数的导数： 实际处理问题： 回归问题： 在回归问题中，直接使用模型预测值作为预测结果，并使用最小均方误差作为损失函数，其中m为样本个数： 二分类问题： 将输出结果通过激活函数，如sigmoid函数得到预测类别的概率，使用对数似然作为损失函数： 场感知分解机 FFMFM的应用场景：给定一组数据，判定用户是否会进行点击。 采用onehot对categorical类型的数据进行编码后，数据会十分稀疏，并且数据维度增大。 以广告分类为例，“Day=26/11/15”、“Day=1/7/14”、“Day=19/2/15”这三个特征都是代表日期的，可以放到同一个field中。同理，商品的末级品类编码生成了550个特征，这550个特征都是说明商品所属的品类，因此它们也可以放到同一个field中。简单来说，同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field，包括用户性别、职业、品类偏好等。 Field-aware Factorization Machine(FFM) 模型场感知说白了可以理解引入了field的概念，FFM把相同性质的特征归于同一个field。 因此，隐向量不仅与特征相关，也与filed相关， 即：对每一维特征分量$x_i$, 针对每一种field $f_j$ , 都会学习一个隐向量$v_{i, f_j}$，与不同的特征关联需要使用不同的隐向量 (而FM每种特征只有一个隐向量)例如，当考虑“Day=26/11/15”这个特征，与“Country”特征和“Ad_type”特征进行关联的时候，需要使用不同的隐向量，而在FM中则使用相同的隐向量。 假设样本的n个特征（已经onehot)属于f个field, 那么FFM二次项有nf个隐向量。 因此，得到： 其中，$f_j$是第j个特征所属的field. 如果隐向量的长度为k, 那么FFM的交叉项参数就有nfk个，远多于FM模型的nk个。此外，由于隐向量于filed有关，FFM的交叉项并不能够像FM那样进行化简，预测复杂度为$O(kn^2)$. FFM的使用：所有的特征必须转换成“field_id:feat_id:value”格式，field_id代表特征所属field的编号，feat_id是特征编号，value是特征的值. 在FFM论文版本里的梯度更新，学习率是通过类似于adagrad自适应的学习率计算的。根据AdaGrad的特点，对于样本比较稀疏的特征，学习率高于样本比较密集的特征，因此每个参数既可以比较快速达到最优，也不会导致验证误差出现很大的震荡。 参考： FM算法详解：https://blog.csdn.net/bitcarmanlee/article/details/52143909 FM计算：https://blog.csdn.net/shenxiaolu1984/article/details/78740481 fm美团：https://tech.meituan.com/deep_understanding_of_ffm_principles_and_practices.html csdn， 推荐算法：https://blog.csdn.net/asd136912/article/details/78318563 美团，FFM: https://tech.meituan.com/deep_understanding_of_ffm_principles_and_practices.html]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Remove Duplicates from Sorted List]]></title>
    <url>%2F2018%2F08%2F26%2FLeetcode-Reverse%20Linked%20List%2F</url>
    <content type="text"><![CDATA[从排序链表中删除重复元素。Description 解题思路：依次比较相邻的两个链表元素，若值相等，则将前一个节点的next引用为后一个节点的后一个节点。使用cur来依次向下遍历元素，最后返回head。 12345678910111213141516171819# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def deleteDuplicates(self, head): """ :type head: ListNode :rtype: ListNode """ cur = head while cur: while cur.next and cur.next.val==cur.val: cur.next = cur.next.next cur = cur.next return head]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine learning]]></title>
    <url>%2F2018%2F08%2F26%2Fmachine-learning%2F</url>
    <content type="text"><![CDATA[本文仅为个人笔记，错误之处敬请包涵。 —— David 这篇介绍不涉及任何公式推导，仅提供一些思路以及相应的资料，感兴趣的可以自行查找资料学习。 书籍推荐理论类： 机器学习，周志华； 统计学习方法，李航； 深度学习，Ian goodfellow; 实践类： 机器学习实战，peter harrington； python机器学习及实践，范淼； 休闲类： 数学之美，吴军； ##博客推荐： 刘建平的博客：https://www.cnblogs.com/pinard/ peghoty的博客：https://blog.csdn.net/itplus poll的笔记：http://www.cnblogs.com/maybe2030/ 监督学习监督学习是指对带标签的数据进行模型学习。 回归回归任务常见的是线性回归linear regression. 注意回归和分类任务的区别，回归的标签值是连续值，而分类的标签值是离散值。 通常我们还会遇到逻辑回归logistic regression, 注意，逻辑回归是用于分类的！ 逻辑回归LR是机器学习中非常基础的算法，因此非常重要。 知识点：交叉熵损失函数，极大似然估计。 相关资料： poll的博客：www.cnblogs.com/maybe2030/p/5494931.html 逻辑回归：https://www.cnblogs.com/Belter/p/6128644.html 逻辑回归推导：http://blog.csdn.net/pakko/article/details/37878837 极大似然估计：http://blog.csdn.net/star_liux/article/details/39666737 SVM通俗来讲，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。 知识点：函数间隔，几何间隔，对偶问题，核函数，软间隔，hinge loss 相关资料： 周志华，机器学习：SVM 图形解释，知乎：https://www.zhihu.com/question/21094489 支持向量机通俗解释，CSDN: http://blog.csdn.net/v_july_v/article/details/7624837 支持向量机通俗导论，july: https://blog.csdn.net/v_july_v/article/details/7624837 KNN给定测试样本，基于某种距离度量找出训练集中与其最靠近的k个训练样本，然后基于这k个邻居的信息来进行预测。K=1时，为最近邻算法。 知识点：k值的选择，距离度量，kd树定义及构建 相关资料： 李航，统计学习，第三章 因子分解机因子分解机主要是考虑了特征之间的关联。 FM主要是为了解决数据稀疏的情况下，（而SVM无法解决稀疏问题），特征怎样组合的问题。 知识点：计算复杂度的推导，梯度计算 相关资料： FM算法详解：https://blog.csdn.net/bitcarmanlee/article/details/52143909 FM计算：https://blog.csdn.net/shenxiaolu1984/article/details/78740481 ###HMM隐马尔可夫模型 隐马模型相对包含内容较多，需要考虑三个基本问题：评估问题，解码问题，和学习问题。 为了更好的理解隐马模型，可以通过http://www.cnblogs.com/skyme/p/4651331.html 这篇文章入门，对隐马模型的隐含状态，可见状态，转换概率等概念有较为清楚的理解。 知识点：评估问题，前向后向算法；解码问题，维特比算法；学习问题，EM算法 相关资料： Cnblog，骰子说明隐马模型：http://www.cnblogs.com/skyme/p/4651331.html 维特比算法说明,及python实现：https://www.cnblogs.com/ylHe/p/6912017.html 数学之美，第5章，隐马模型，BW算法 无监督学习无监督学习是对无标签的数据进行模型学习。 聚类最基本的算法是K-means, 通过不断的迭代得到最终结果。 知识点：K-means的步骤，k值的确定（注意，这里的k和knn中的k概念不同），如何确定k个初始类簇中心点； 频繁项集和关联分析频繁项集主要是指经常出现在一块的物品的集合，关联分析是指从大规模数据中寻找物品间的隐含关系。在寻找频繁项集的过程中，主要有两种算法，Apriori以及FP-growth. 知识点：清楚相关概念：包括项，项集，事务，关联分析，频繁项集，关联规则，支持度，置信度；熟悉Apriori，和FP-growth算法 相关资料： FP-tree算法的实现：https://www.cnblogs.com/zhangchaoyang/articles/2198946.html 序列模式序列模式属于频繁模式的进阶，频繁模式是挖掘事务中的频繁项，序列模式是挖掘频繁的子序列。可以认为，事务是项的无序集合，序列是事务的有序集合。因此，可以和频繁项集对比学习。 知识点：序列的概念，相关算法实现：GSP, SPADE, PrefixSpan,… 相关资料： 数据挖掘关联分析：https://www.cnblogs.com/beaver-sea/p/4743167.html 序列模式，GSP：http://blog.csdn.net/rongyongfeikai2/article/details/40478335 刘建平，PrefixSpan: https://www.cnblogs.com/pinard/p/6323182.html 基本分析一些在机器学习中常用到的基本概念。 梯度下降梯度下降是机器学习中必然掌握的核心概念，需要能够继续推导。为了更好的理解梯度下降，建议从梯度的概念着手。 知识点：梯度下降的概念，批量梯度下降，随机梯度下降，小批量梯度下降（最常用），反向传播 相关资料： Poll的笔记，梯度下降法的三种形式BGD、SGD以及MBGD：https://www.cnblogs.com/maybe2030/p/5089753.html 反向传播，代入数值具体计算：https://www.cnblogs.com/charlotte77/p/5629865.html SGD, BGD, MBGD的比较：https://blog.csdn.net/tsyccnh/article/details/76136771 超参数所谓的模型配置，一般统称为模型的超参数（Hyperparameters），比如KNN算法中的K值，SVM中不同的核函数（Kernal）等。多数情况下，超参数等选择是无限的。在有限的时间内，除了可以验证人工预设几种超参数组合以外，也可以通过启发式的搜索方法对超参数组合进行调优。称这种启发式的超参数搜索方法为网格搜索。 实战中调参即指对超参数的调参。 损失函数损失函数用来评价模型的预测值和真实值不一样的程度，损失函数越小，通常模型的性能越好。 把最大化或者最小化的函数称为目标函数，把需要最小化的函数称为代价函数或者损失函数，因为我们的优化是最小化代价或者损失。 损失函数分为经验风险损失函数和结构风险损失函数。简单来说，结构风险损失函数就是在经验风险损失函数的基础上加上了正则项。 常见损失函数包括：绝对值损失函数，对数损失函数（通常用于分类），平方损失函数（通常用于回归）… 相关资料： 交叉熵：http://blog.csdn.net/sinat_29819401/article/details/58716834 常见的几种损失函数：https://www.cnblogs.com/hejunlin1992/p/8158933.html 知乎，目标函数，损失函数和代价函数的区别：https://www.zhihu.com/question/52398145 算法常用指标分类常见指标包括AUC，ROC，召回率，准确率.. 回归常见指标包括RMSE… 相关资料： poll的笔记：www.cnblogs.com/maybe2030/p/5375175.html AUC，ROC：https://www.jianshu.com/p/c61ae11cc5f6 特征选择和特征抽取在机器学习中特征是非常重要的，经常需要进行特征工程，因此特征选择和特征抽取相当关键。 特征抽取（Feature Extraction）:Creatting a subset of new features by combinations of the exsiting features.也就是说，特征抽取后的新特征是原来特征的一个映射。 特征选择（Feature Selection）:choosing a subset of all the features(the ones more informative)。也就是说，特征选择后的特征是原来特征的一个子集。 特征抽取算法包括PCA，LDA… 特征选择算法包括三类：Filter, Wrapper, and Embedded 知识点：特征选择和特征抽取的意义，PCA, SVD, … 相关资料： csdn博客：http://blog.csdn.net/shenxiaoming77/article/details/50555054 周志华，机器学习，11章：特征选择和稀疏学习 伯乐在线，主成分分析原理详解：http://blog.jobbole.com/109015/ 连续值，异常值，缺失值处理知识点：连续值转换为离散值，异常值检测，缺失值填充 L1和L2正则化知识点：了解正则化的目标，L1和L2正则的作用，等值线图的含义 相关资料： 博客：http://blog.csdn.net/zouxy09/article/details/24971995 L2正则化导致参数较小：https://blog.csdn.net/jinping_shi/article/details/52433975 等值线图形的解释：https://blog.csdn.net/weixin_39845112/article/details/80114918 过拟合过拟合的一些解决方法： 增加样本数据量； 正则化，L1和L2正则； 神经网络中，dropout方法。就是每层网络的训练，随机的让一半神经元不工作。达到防止过拟合的目的。 决策树中可以用剪枝操作 提升方法：early stopping，当训练集合的误差降低，但是验证集合的误差增加时，则停止训练，同时返回具有最小验证集合误差的神经网络； Data augmentation：创建假数据并增加到训练集中 交叉验证训练集training set：用以建立模型，是用来训练模型或确定模型参数的 测试集testing set: 用来评估模型对未知样本进行预测时的精确度，即泛化能力 验证集validation set: 验证集是原始训练集的子集，用来做模型选择 集成学习集成学习在机器学习中有非常重要的地位，因此需要深入的研究。集成学习重要分为三种，Boosting, Bagging 和Stacking。 Boosting包括了GBDT, XGBoost, AdaBoost… Bagging包含了随机森林 GBDTGBDT中的树都是回归树，不是分类树。 每一轮弱学习器的目标是使预测值接近残差， 强学习器的作用是累加前面所有的弱学习器结果，使预测值接近实际值。 残差是上一轮强学习器的预测结果和实际值的差值或者用损失函数的负梯度方向表示。 Gradient Boosting 是GBDT的核心思想，它相比与 Boosting Tree 的主要改进在于: Boosting Tree 对于每一轮基学习器，拟合的是当前模型与标签值的残差 ， GBDT 对于每一轮基学习器，拟合的是当前模型与标签值的残差的负梯度。 知识点：CART回归树，负梯度，GBDT多分类，GBDT与XGBoost的区别 相关资料： 入门介绍：http://blog.csdn.net/w28971023/article/details/8240756 刘建平：https://www.cnblogs.com/pinard/p/6140514.html 集成学习总结：https://xijunlee.github.io/2017/06/03/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/ GBDT和XGBoost比较：https://blog.csdn.net/panda_zjd/article/details/71577463 面试题整理：https://blog.csdn.net/qq_28031525/article/details/70207918 GBDT构建特征：https://blog.csdn.net/shine19930820/article/details/71713680#2-gbdt构建新的特征思想 GBDT与Boosting Tree的区别：https://blog.csdn.net/Liangjun_Feng/article/details/80142724?spm=5176.11104323.5600004.1.444e717dXEDRaI XGBoost是对GBDT的一种改进，也是实战中常用的模型，因此需要加深理解，建议直接看陈天奇大神的论文，以及他的PPT介绍。 知识点：了解XGBoost模型中各个参数的含义 Lightgbm是另一种常见的模型，由微软提出。 知识点：了解Lightgbm中各个参数的含义，了解lightgbm和xgboost的比较 DART将drop out引入到boosting模型中的一种算法。解决over specilization的问题，即初始的第一棵树对整体方向起关键作用，而后续的树一直处于弥补残差的状态，并且只影响一小部分样本，这样后面的树对整体的贡献也在逐渐的弱化。 相关资料： Csdn: https://blog.csdn.net/Yongchun_Zhu/article/details/78745529 AdaBoostadaboost算法的核心思想就是由分类效果较差的弱分类器逐步的强化成一个分类效果较好的强分类器。而强化的过程，就是逐步的改变样本权重，样本权重的高低，代表其在分类器训练过程中的重要程度。 相关资料： 深度剖析adaboost: http://blog.csdn.net/autocyz/article/details/51305999 一问读懂adaboost: www.xtecher.com/Xfeature/view?aid=8109 随机森林和决策树决策树的关键是选择最优划分属性，了解三种划分属性的方式，如ID3, C4.5, GINI。 Random Forest（随机森林）是Bagging的扩展变体，它在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机特征选择，因此可以概括RF包括四个部分： 1、随机选择样本（放回抽样）； 2、构建决策树（CART树）； 3、随机选择特征； 4、随机森林投票（平均）。 知识点：决策树划分属性的方式，剪枝，Bootstrapping，随机森林进行特征选择 相关资料： 周志华，机器学习； CART回归树, 李航，第五章决策树； 随机森林，面试：https://blog.csdn.net/qq_28031525/article/details/70207918 特征选择：https://blog.csdn.net/banbuduoyujian/article/details/60328474 stacking在kaggle比赛中常用的一种算法，将训练好的所有基模型对整个训练集进行预测，第j个基模型对第i个训练样本的预测值将作为新的训练集中第i个样本的第j个特征值(预测值变成了特征值），最后基于新的训练集进行训练。 相关资料： 从boosting到stacking: https://baijiahao.baidu.com/s?id=1576971033986792968&amp;wfr=spider&amp;for=pc csdn博客，集成学习总结：https://blog.csdn.net/willduan1/article/details/73618677 stack图形说明：https://www.leiphone.com/news/201709/zYIOJqMzR0mJARzj.html Stacking, kaggle: https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard 数据处理数据归一化归一化的作用： 归一化后加快了梯度下降求最优解的速度 归一化有可能提高精度 中心化Zero-centered/Mean-substraction： 指变量减去它的均值 标准化Standardization/Normalization 指变量减去均值后（中心化），再除以标准差； 意义：数据中心化和标准化的作用的取消在回归分析中由于不同特征量纲不同或者数值相差较大所引起的误差。 注意：归一化和标准化都是线性变换。 相关资料： 归一化：http://www.cnblogs.com/LBSer/p/4440590.html Batch Normalization数据预处理，相当于深度学习中的数据归一化。 普通机器学习的归一化只针对输入层，而BN针对中间每一层输入进行归一化。 归一化的目的是使的数据的分布集中在激活函数的敏感区域（即中间区域）。 BN层就是对深度学习的中间层的输入数据进行归一化，解决在训练过程中，中间层数据分布发生改变的情况，使得每一层的分布相似，同时保留原来每层训练得到的特征分布（通过变换重构）。 相关资料： Batch normalization 学习笔记：https://blog.csdn.net/hjimce/article/details/50866313 白化 whitening白化就是深度学习里的PCA. 分为PCA白化和ZCA白化。 白化的目的是去除输入数据的冗余信息。假设训练数据是图像，由于图像中相邻像素之间具有很强的相关性，所以用于训练时输入是冗余的； 白化的目的就是降低输入的冗余性。 白化和PCA的关系： PCA如果不降维，而是仅仅使用PCA求出特征向量，然后把数据X映射到新的特征空间，这样的一个映射过程，其实就是满足了我们白化的第一个性质：除去特征之间的相关性。因此白化算法的实现过程，第一步操作就是PCA，求出新特征空间中X的新坐标，然后再对新的坐标进行方差归一化操作。 相关资料： 白化，hjimce: https://blog.csdn.net/hjimce/article/details/50864602 分类不均衡问题分类不均衡问题是指：分类任务中不同类别的训练样例数目差别很大的情况。 对样例数目较多的类别进行欠抽样（undersampling)（1,2,3均是删去多数类样本的方法）： 随机欠采样：最简单的办法是从多数类中随机抽取样本从而减少多数类样本的数量，使数据达到平衡； Edited Nearest Neighbor(ENN): 在多数类的样本中，如果该样本周围的K近邻样本都是少数类，则将该多数类样本删除； Tomek Link Removal:如果有两个不同类别的样本，它们的最近邻都是对方，那么A,B就是Tomek link。我们要做的就是将所有Tomek link都删除掉，即将组成Tomek link的两个样本，如果有一个属于多数类样本，就将该多数类样本删除掉。 代表性算法EasyEnsemble,利用集成学习机制，将该类别划分为若干个集合供不同学习器使用，对于每个学习器来说都是欠采样，但从全局来看却不会丢失重要信息。 对样例数目较少的类别进行过抽样（oversampling)： 过抽样不是简单的对样本进行重复抽样，否则会招致严重的过拟合，过采样的代表算法是SMOTE,基本思想是对少数类样本进行分析并根据少数类样本人工合成新样本添加到数据集中。简单来说，人工合成的样本是每个少数类样本与其K近邻的连线上的点。 再缩放（再平衡）技术 即给少数类较多的权重，而给多数类更少的权重，使得少数类判别错误的损失大于多数类判别错误的损失，其中，缩放因子为类别样例数目的比值 相关资料： SMOTE算法：https://blog.csdn.net/jiede1/article/details/70215477 分类不均衡：https://blog.csdn.net/ly_ysys629/article/details/72846200 分类不均衡方法综述: http://lib.csdn.net/article/machinelearning/41294]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F08%2F26%2Fhello-world%2F</url>
    <content type="text"><![CDATA[hexo 更新hexo clean hexo g -d]]></content>
      <categories>
        <category>test</category>
      </categories>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
</search>
