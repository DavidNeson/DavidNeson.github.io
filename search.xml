<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[线性回归和逻辑回归的比较]]></title>
    <url>%2F2018%2F09%2F11%2Flinear%20and%20logistic%20learning%2F</url>
    <content type="text"><![CDATA[线性回归 用一组变量的（特征）的线性组合，来建立与结果之间的关系。 模型表达：$y(x, w)=w_0+w_1x_1+…+w_nx_n$ 逻辑回归逻辑回归用于分类，而不是回归。 在线性回归模型中，输出一般是连续的， 对于每一个输入的x，都有一个对应的输出y。因此模型的定义域和值域都可以是无穷。 但是对于逻辑回归，输入可以是连续的[-∞, +∞]，但输出一般是离散的，通常只有两个值{0, 1}。 这两个值可以表示对样本的某种分类，高/低、患病/ 健康、阴性/阳性等，这就是最常见的二分类逻辑回归。因此，从整体上来说，通过逻辑回归模型，我们将在整个实数范围上的x映射到了有限个点上，这样就实现了对x的分类。因为每次拿过来一个x，经过逻辑回归分析，就可以将它归入某一类y中。 逻辑回归与线性回归的关系可以认为逻辑回归的输入是线性回归的输出，将逻辑斯蒂函数（Sigmoid曲线）作用于线性回归的输出得到输出结果。 线性回归y = ax + b, 其中a和b是待求参数； 逻辑回归p = S(ax + b), 其中a和b是待求参数， S是逻辑斯蒂函数，然后根据p与1-p的大小确定输出的值，通常阈值取0.5，若p大于0.5则归为1这类。 具体的： 线性函数如下： 构造预测函数： 逻辑回归的损失函数： 逻辑回归采用交叉熵作为代价函数，即对数损失函数。能够有效避免梯度消失. 对数损失函数（logarithmic loss function) 或对数似然损失函数(log-likehood loss function)： $$L(Y,P(Y|X))=−logP(Y|X)$$ 逻辑回归中，采用的是负对数损失函数。如果损失函数越小，表示模型越好。 极大似然估计： 极大似然原理的直观想法是，一个随机试验如有若干个可能的结果A，B，C，… ，若在一次试验中，结果A出现了，那么可以认为实验条件对A的出现有利，也即出现的概率P(A)较大。一般说来，事件A发生的概率与某一未知参数θ有关， θ取值不同，则事件A发生的概率也不同，当我们在一次试验中事件A发生了，则认为此时的θ值应是一切可能取值中使P(A|θ）达到最大的那一个，极大似然估计法就是要选取这样的θ值作为参数的估计值，使所选取的样本在被选的总体中出现的可能性为最大。 在逻辑回归中目标函数均为最大化条件概率p(y|x),其中x是输入样本 似然：选择参数使似然概率p(y|x,)最大，y是实际标签 过程：目标函数→单个样本的似然概率→所有样本的似然概率→log变换, 将累乘变成累加→负号, 变成损失函数 选择一组参数使得实验结果具有最大概率。 损失函数的由来： 已知估计函数为： 则似然概率分布为（即输出值为判断为1的概率，但在输出标签值时实际只与0.5作比较）： 可以写成概率一般式： 由最大似然估计原理，我们可以通过m个训练样本值，来估计出值，使得似然函数值（所有样本的似然函数之积）最大 求log: 取负数，得损失函数： 逻辑回归参数迭代，利用反向传播进行计算： 上面这个过程计算的是单个样本对wj的梯度更新。 为什么逻辑回归采用似然函数，而不是平方损失函数？可以从两个角度理解。 交叉熵损失函数的好处是可以克服方差代价函数更新权重过慢的问题（针对激活函数是sigmoid的情况）。 原因是其梯度里面不在包含对sigmoid函数的导数： 而如果使用的是平方损失函数加sigmoid函数，则计算梯度时： 会包含sigmoid的导数(sigmoid的导数值始终小于1），使梯度下降变慢。 图1 最小二乘作为逻辑回归模型的损失函数(非凸），theta为待优化参数 ​ 图2 最大似然作为逻辑回归模型的损失函数，theta为待优化参数 逻辑回归为什么使用sigmoid函数也可以从两点来进行理解 Sigmoid 函数自身的性质 因为这是一个最简单的，可导的，0-1阶跃函数 sigmoid 函数连续，单调递增 sigmiod 函数关于（0，0.5） 中心对称 对sigmoid函数求导简单 逻辑回归函数的定义 因此, 逻辑回归返回的概率是指判别为1类的概率. 逻辑回归和SVM的异同点相同点： 第一，LR和SVM都是分类算法。 第二，如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性的。 第三，LR和SVM都是监督学习算法。 第四，LR和SVM都是判别模型。 判别模型会生成一个表示P(Y|X)的判别函数（或预测模型），而生成模型先计算联合概率p(Y,X)然后通过贝叶斯公式转化为条件概率。简单来说，在计算判别模型时，不会计算联合概率，而在计算生成模型时，必须先计算联合概率。 不同点： 第一，本质上是其loss function不同 逻辑回归的损失函数是交叉熵函数： SVM的损失函数： 逻辑回归方法基于概率理论，假设样本为1的概率可以用sigmoid函数来表示，然后通过极大似然估计的方法估计出参数的值； 支持向量机基于几何间隔最大化原理，认为存在最大几何间隔的分类面为最优分类面； 第二，支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用）。 第三，在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。 这个问题理解起来非常简单。分类模型的结果就是计算决策面，模型训练的过程就是决策面的计算过程。通过上面的第二点不同点可以了解，在计算决策面时，SVM算法里只有少数几个代表支持向量的样本参与了计算，也就是只有少数几个样本需要参与核计算（即kernal machine解的系数是稀疏的）。然而，LR算法里，每个样本点都必须参与决策面的计算过程，也就是说，假设我们在LR里也运用核函数的原理，那么每个样本点都必须参与核计算，这带来的计算复杂度是相当高的。所以，在具体应用时，LR很少运用核函数机制。 第四，SVM的损失函数就自带正则 参考： Poll的博客：www.cnblogs.com/maybe2030/p/5494931.html 逻辑回归：https://www.cnblogs.com/Belter/p/6128644.html 逻辑回归推导：http://blog.csdn.net/pakko/article/details/37878837 极大似然估计：http://blog.csdn.net/star_liux/article/details/39666737 刘建平：https://www.cnblogs.com/pinard/p/6035872.html 逻辑回归和SVM的比较：https://www.cnblogs.com/zhizhan/p/5038747.html]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L1和L2正则化]]></title>
    <url>%2F2018%2F09%2F11%2FL1%20and%20L2%2F</url>
    <content type="text"><![CDATA[L0范数，L1范数，L2范数L0范数是指向量中非0元素的个数。 如果我们用L0范数来规则化一个参数矩阵W的话（正则项），就是希望W的大部分元素都是0。换句话说，让参数W是稀疏的。 L1范数是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。 为什么要稀疏？特征选择(Feature Selection)： 稀疏规则化它能实现特征的自动选择。一般来说，一个样本的大部分特征都是和最终的输出没有关系或者不提供任何信息的，在最小化目标函数的时候考虑这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息被考虑反而干扰了对正确标签的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。 可解释性(Interpretability)： 另一个青睐于稀疏的理由是，模型更容易解释。例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。假设我们这个是个回归模型：$y=w_1x_1+w_2x_2+…+w_1000x_1000+b$ (为了让y限定在[0,1]的范围，一般还得加个Logistic函数）。通过学习，如果最后学习到的w就只有很少的非零元素，例如只有5个非零的$w_i$，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个$w_i$都非0，医生面对这1000种因素，累觉不爱。 L2范数是指向量各元素的平方和然后求平方根。作用是改善过拟合。 在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减weight decay”。 实际上，对于L1和L2规则化的代价函数来说，我们可以写成以下形式： 为什么L1正则化可以取稀疏解等值线图的理解： 目标函数由两部分组成：原始的损失函数和正则项 其中$J_0$是原始的损失函数，是关于权重w的函数，上图中的等值线图考虑了只有两个权重系数的情况。 右上方的彩色圆圈是关于原始损失函数的等值线图，可以想象成是一个曲面压平后的结果。 因此，每一圈上的每个坐标带入原始损失函数得到的损失值是相同的。 由于原始损失函数是凸函数，因此紫色中心是取值最小的，也就是说，不加正则项，我们应该取紫色点的权重坐标。而越往外扩散，原始损失函数的取值越大。 注意：等值线图并没有画全，还可以继续向外扩散。 正则项相当于对原始损失函数做了约束，令L代表正则项，因此黑色线圈表示满足约束的权重坐标。 如果是L1， 则$L=|w_1| + |w_2|$， 若为L2， 则$L = |w_1|^2 + |w_2|^2$ 因此，为了要满足约束，我们要取黑色线与彩色线的交点，但是同时为了使彩色线的取值尽可能小，我们需要取尽可能靠近紫色线方向的线圈，因此最终取图中的交点。否则，如果和黑色线上别的交点相交，彩色线圈会往外移，导致取值增加。 为什么L2可以获得平滑的解？(从理论角度)假设原始损失函数为： 此时，进行梯度迭代的公式为： 如果加上L2正则项，则梯度更新公式变为： 因此，与未添加L2的情况相比，参数每次迭代都要乘以一个小于1的因子，从而使得参数不断减小。 为什么正则化/参数较小可以防止过拟合？ 直观理解就是：正则项λ越大，那么权重就会越小，使得神经网络变得简单。 根据激活函数理解，假设激活函数是tanh: 可以发现，当z值较小时，激活函数的部分几乎为线性形状。 如果正则化项λ很大，那么权重就会很小，因此相对来说，z=wx+b也会很小，因此在这段范围内，激活函数呈线性，和线性回归类似，因此，当每层网络都是线性网络，那么整个网络都是一个线性网络，无法适用于复杂决策，不会发生过拟合。 过拟合的时候，拟合函数的系数往往非常大，为什么？如下图所示，过拟合，就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。 而正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。 贝叶斯派解释： 加2范数正则等价于加了高斯分布的先验，加1范数正则相当于加拉普拉斯分布先验。 为什么L2得到的系数趋于相同L2正则化将系数向量的L2范数添加到了损失函数中。由于L2惩罚项中系数是二次方的，这使得L2和L1有着诸多差异，最明显的一点就是，L2正则化会让系数的取值变得平均。对于关联特征，这意味着他们能够获得更相近的对应系数。还是以Y=X1+X2为例，假设X1和X2具有很强的关联，如果用L1正则化，不论学到的模型是Y=X1+X2还是Y=2X1，惩罚都是一样的，都是2alpha。但是对于L2来说，第一个模型的惩罚项是2alpha，但第二个模型的是4*alpha。可以看出，系数之和为常数时，各系数相等时惩罚是最小的，所以才有了L2会让各个系数趋于相同的特点。 总结因此，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。 L1正则假设参数的先验分布是Laplace分布，可以保证模型的稀疏性，也就是某些参数等于0； L2正则假设参数的先验分布是Gaussian分布，可以保证模型的稳定性，也就是参数的值不会太大或太小； 在实际使用中，如果特征是高维稀疏的，则使用L1正则；如果特征是低维稠密的，则使用L2正则。 参考： 百度知道：https://zhidao.baidu.com/question/1864240911512690707.html 博客：http://blog.csdn.net/zouxy09/article/details/24971995 L2正则化导致参数较小：https://blog.csdn.net/jinping_shi/article/details/52433975 等值线图形的解释：https://blog.csdn.net/weixin_39845112/article/details/80114918]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>过拟合</tag>
        <tag>正则化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[因子分解机 FM和FFM]]></title>
    <url>%2F2018%2F09%2F11%2FFM%20%26%20FFM%2F</url>
    <content type="text"><![CDATA[因子分解机 Factorization Machine因子分解机主要是考虑了特征之间的关联。 FM主要是为了解决数据稀疏的情况下，（而SVM无法解决稀疏问题），特征怎样组合的问题。 数据稀疏是指数据的维度很大，但是其中为0的维度很多。推荐系统是常见应用场景，原因是推荐系统中类别属性（如商品id）比较多，每一种类别属性经过onehot处理后会产生大量值为0的特征，导致样本变得稀疏，而FM就可以解决这种样本稀疏的问题。 因子分解机FM算法可以处理如下三类问题： 普通线性模型我们将各个特征独立考虑，并没有考虑特征之间的相互关系。 FM模型为了表述特征间的相关性，我们采用多项式模型，将特征$x_i$和$x_j$的组合用$x_ix_j$表示，只讨论二阶多项式模型： 其中，n表示样本的维度（是已经进行onehot以后的特征数量），$x_i$表示第i个特征，(如果是类别变量，那么onehot后只有一个维度值为1，其余维度值为0，因此在这种情况下$x_i$的值通常是取0和1，而对于一般的数值维度，$x_i$的值对应原来的数值)， $w_{ij}$是组合参数，代表组合特征的重要性，注意：$w_{ij}$和$w_{ji}$是相等的，因此组合特征部分相关参数共有$(n-1)+(n-2)+…+1=n(n-1)/2$ 注意到，在数据稀疏的情况下，满足特征$x_i$和$x_j$都不为0的情况很少，因此$w_{ij}$很难训练。 为了求解组合参数$w_{ij}$, 对每个特征分量$x_i$引入k维（k远小于n) 的辅助向量$v_i=(v_{i1}, v_{i2},…,v_{ik})$, 然后利用向量内积的结果$v_iv_j^T$来表示原来的组合参数$w_{ij}$ 实际上，辅助向量可以理解为是特征分量的另一种表示形式，类似于词向量的表示形式，但是和词向量存在区别。词向量中是将一个单词转换为向量表示形式，而单词是固定的，因此一个单词对应一个词向量；而在FM中，我们是将一个类别特征（注意，这是onehot前的特征）转换为一个向量，但由于该类别特征可以有多种取值，并且每种取值对应一个向量(也就是上面将类别特征onehot以后，每个特征分量对应一个辅助向量），因此，FM中确实是将一个类别特征转换为了向量形式，只不过向量会根据特征的取值发生变化。 此时，组合参数$w_{ij}$组成的矩阵可以表示为： 将组合参数进行分解的好处： 从原来要求n(n-1)/2个组合参数变成了求矩阵V，参数数量变为n*k. 削弱了高阶参数间的独立性：k越大（即对特征分量的表征能力越强），高阶参数间独立性越强，模型越精细；k越小，泛化能力越强， 因此实际问题选择较小的k可以克服稀疏数据的问题，并获得较好的预测效果。 因此时间复杂度从O(n^2)变成了O(kn) 此时，分解机的表示形式变为： 注意第二项，下标j的循环从i+1开始。 使用辅助向量乘积表示组合参数的原理： 通常，由于数据稀疏，本来组合参数是学习不到的，但是我们可以通过特征i与其他特征的数据的关系，特征j和其他特征的关系，分别学习到特征i和特征j的对应的辅助向量$v_i$和$v_j$,这样利用$v_iv_j^T$来表示$w_{ij}$，便可以解决数据稀疏带来的问题。 计算模型的预测值： 在计算模型时，只需要考虑计算量最大的二次项： 可以先把标量$x_i$和对应的辅助向量$v_i$相乘，并记录下来，得到$u_i=x_iv_i$, 注意$x_i$只是标量。 对于n个元素，共需要n*k次乘法，于是二元项变为：注意：公式中的r即为k,即辅助向量的维度 把上式凑成和的平方： 化简的原理是将整个对称矩阵W除去对角线上的数值，由于对称，再除以2得到原来的上三角矩阵。 括号内，两部分计算量均为O(n),因此整体计算量为O(kn)。 梯度下降求解模型参数： SGD中，需要计算两种导数： 预测值对一元参数的导数： 预测值对二元参数的导数： 实际处理问题： 回归问题： 在回归问题中，直接使用模型预测值作为预测结果，并使用最小均方误差作为损失函数，其中m为样本个数： 二分类问题： 将输出结果通过激活函数，如sigmoid函数得到预测类别的概率，使用对数似然作为损失函数： 场感知分解机 FFMFM的应用场景：给定一组数据，判定用户是否会进行点击。 采用onehot对categorical类型的数据进行编码后，数据会十分稀疏，并且数据维度增大。 以广告分类为例，“Day=26/11/15”、“Day=1/7/14”、“Day=19/2/15”这三个特征都是代表日期的，可以放到同一个field中。同理，商品的末级品类编码生成了550个特征，这550个特征都是说明商品所属的品类，因此它们也可以放到同一个field中。简单来说，同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field，包括用户性别、职业、品类偏好等。 Field-aware Factorization Machine(FFM) 模型场感知说白了可以理解引入了field的概念，FFM把相同性质的特征归于同一个field。 因此，隐向量不仅与特征相关，也与filed相关， 即：对每一维特征分量$x_i$, 针对每一种field $f_j$ , 都会学习一个隐向量$v_{i, f_j}$，与不同的特征关联需要使用不同的隐向量 (而FM每种特征只有一个隐向量)例如，当考虑“Day=26/11/15”这个特征，与“Country”特征和“Ad_type”特征进行关联的时候，需要使用不同的隐向量，而在FM中则使用相同的隐向量。 假设样本的n个特征（已经onehot)属于f个field, 那么FFM二次项有nf个隐向量。 因此，得到： 其中，$f_j$是第j个特征所属的field. 如果隐向量的长度为k, 那么FFM的交叉项参数就有nfk个，远多于FM模型的nk个。此外，由于隐向量于filed有关，FFM的交叉项并不能够像FM那样进行化简，预测复杂度为$O(kn^2)$. FFM的使用：所有的特征必须转换成“field_id:feat_id:value”格式，field_id代表特征所属field的编号，feat_id是特征编号，value是特征的值. 在FFM论文版本里的梯度更新，学习率是通过类似于adagrad自适应的学习率计算的。根据AdaGrad的特点，对于样本比较稀疏的特征，学习率高于样本比较密集的特征，因此每个参数既可以比较快速达到最优，也不会导致验证误差出现很大的震荡。 参考： FM算法详解：https://blog.csdn.net/bitcarmanlee/article/details/52143909 FM计算：https://blog.csdn.net/shenxiaolu1984/article/details/78740481 fm美团：https://tech.meituan.com/deep_understanding_of_ffm_principles_and_practices.html csdn， 推荐算法：https://blog.csdn.net/asd136912/article/details/78318563 美团，FFM: https://tech.meituan.com/deep_understanding_of_ffm_principles_and_practices.html]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Remove Duplicates from Sorted List]]></title>
    <url>%2F2018%2F08%2F26%2FLeetcode-Reverse%20Linked%20List%2F</url>
    <content type="text"><![CDATA[从排序链表中删除重复元素。Description 解题思路：依次比较相邻的两个链表元素，若值相等，则将前一个节点的next引用为后一个节点的后一个节点。使用cur来依次向下遍历元素，最后返回head。 12345678910111213141516171819# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def deleteDuplicates(self, head): """ :type head: ListNode :rtype: ListNode """ cur = head while cur: while cur.next and cur.next.val==cur.val: cur.next = cur.next.next cur = cur.next return head]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine learning]]></title>
    <url>%2F2018%2F08%2F26%2Fmachine-learning%2F</url>
    <content type="text"><![CDATA[本文仅为个人笔记，错误之处敬请包涵。 —— David 这篇介绍不涉及任何公式推导，仅提供一些思路以及相应的资料，感兴趣的可以自行查找资料学习。 书籍推荐理论类： 机器学习，周志华； 统计学习方法，李航； 深度学习，Ian goodfellow; 实践类： 机器学习实战，peter harrington； python机器学习及实践，范淼； 休闲类： 数学之美，吴军； ##博客推荐： 刘建平的博客：https://www.cnblogs.com/pinard/ peghoty的博客：https://blog.csdn.net/itplus poll的笔记：http://www.cnblogs.com/maybe2030/ 监督学习监督学习是指对带标签的数据进行模型学习。 回归回归任务常见的是线性回归linear regression. 注意回归和分类任务的区别，回归的标签值是连续值，而分类的标签值是离散值。 通常我们还会遇到逻辑回归logistic regression, 注意，逻辑回归是用于分类的！ 逻辑回归LR是机器学习中非常基础的算法，因此非常重要。 知识点：交叉熵损失函数，极大似然估计。 相关资料： poll的博客：www.cnblogs.com/maybe2030/p/5494931.html 逻辑回归：https://www.cnblogs.com/Belter/p/6128644.html 逻辑回归推导：http://blog.csdn.net/pakko/article/details/37878837 极大似然估计：http://blog.csdn.net/star_liux/article/details/39666737 SVM通俗来讲，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。 知识点：函数间隔，几何间隔，对偶问题，核函数，软间隔，hinge loss 相关资料： 周志华，机器学习：SVM 图形解释，知乎：https://www.zhihu.com/question/21094489 支持向量机通俗解释，CSDN: http://blog.csdn.net/v_july_v/article/details/7624837 支持向量机通俗导论，july: https://blog.csdn.net/v_july_v/article/details/7624837 KNN给定测试样本，基于某种距离度量找出训练集中与其最靠近的k个训练样本，然后基于这k个邻居的信息来进行预测。K=1时，为最近邻算法。 知识点：k值的选择，距离度量，kd树定义及构建 相关资料： 李航，统计学习，第三章 因子分解机因子分解机主要是考虑了特征之间的关联。 FM主要是为了解决数据稀疏的情况下，（而SVM无法解决稀疏问题），特征怎样组合的问题。 知识点：计算复杂度的推导，梯度计算 相关资料： FM算法详解：https://blog.csdn.net/bitcarmanlee/article/details/52143909 FM计算：https://blog.csdn.net/shenxiaolu1984/article/details/78740481 ###HMM隐马尔可夫模型 隐马模型相对包含内容较多，需要考虑三个基本问题：评估问题，解码问题，和学习问题。 为了更好的理解隐马模型，可以通过http://www.cnblogs.com/skyme/p/4651331.html 这篇文章入门，对隐马模型的隐含状态，可见状态，转换概率等概念有较为清楚的理解。 知识点：评估问题，前向后向算法；解码问题，维特比算法；学习问题，EM算法 相关资料： Cnblog，骰子说明隐马模型：http://www.cnblogs.com/skyme/p/4651331.html 维特比算法说明,及python实现：https://www.cnblogs.com/ylHe/p/6912017.html 数学之美，第5章，隐马模型，BW算法 无监督学习无监督学习是对无标签的数据进行模型学习。 聚类最基本的算法是K-means, 通过不断的迭代得到最终结果。 知识点：K-means的步骤，k值的确定（注意，这里的k和knn中的k概念不同），如何确定k个初始类簇中心点； 频繁项集和关联分析频繁项集主要是指经常出现在一块的物品的集合，关联分析是指从大规模数据中寻找物品间的隐含关系。在寻找频繁项集的过程中，主要有两种算法，Apriori以及FP-growth. 知识点：清楚相关概念：包括项，项集，事务，关联分析，频繁项集，关联规则，支持度，置信度；熟悉Apriori，和FP-growth算法 相关资料： FP-tree算法的实现：https://www.cnblogs.com/zhangchaoyang/articles/2198946.html 序列模式序列模式属于频繁模式的进阶，频繁模式是挖掘事务中的频繁项，序列模式是挖掘频繁的子序列。可以认为，事务是项的无序集合，序列是事务的有序集合。因此，可以和频繁项集对比学习。 知识点：序列的概念，相关算法实现：GSP, SPADE, PrefixSpan,… 相关资料： 数据挖掘关联分析：https://www.cnblogs.com/beaver-sea/p/4743167.html 序列模式，GSP：http://blog.csdn.net/rongyongfeikai2/article/details/40478335 刘建平，PrefixSpan: https://www.cnblogs.com/pinard/p/6323182.html 基本分析一些在机器学习中常用到的基本概念。 梯度下降梯度下降是机器学习中必然掌握的核心概念，需要能够继续推导。为了更好的理解梯度下降，建议从梯度的概念着手。 知识点：梯度下降的概念，批量梯度下降，随机梯度下降，小批量梯度下降（最常用），反向传播 相关资料： Poll的笔记，梯度下降法的三种形式BGD、SGD以及MBGD：https://www.cnblogs.com/maybe2030/p/5089753.html 反向传播，代入数值具体计算：https://www.cnblogs.com/charlotte77/p/5629865.html SGD, BGD, MBGD的比较：https://blog.csdn.net/tsyccnh/article/details/76136771 超参数所谓的模型配置，一般统称为模型的超参数（Hyperparameters），比如KNN算法中的K值，SVM中不同的核函数（Kernal）等。多数情况下，超参数等选择是无限的。在有限的时间内，除了可以验证人工预设几种超参数组合以外，也可以通过启发式的搜索方法对超参数组合进行调优。称这种启发式的超参数搜索方法为网格搜索。 实战中调参即指对超参数的调参。 损失函数损失函数用来评价模型的预测值和真实值不一样的程度，损失函数越小，通常模型的性能越好。 把最大化或者最小化的函数称为目标函数，把需要最小化的函数称为代价函数或者损失函数，因为我们的优化是最小化代价或者损失。 损失函数分为经验风险损失函数和结构风险损失函数。简单来说，结构风险损失函数就是在经验风险损失函数的基础上加上了正则项。 常见损失函数包括：绝对值损失函数，对数损失函数（通常用于分类），平方损失函数（通常用于回归）… 相关资料： 交叉熵：http://blog.csdn.net/sinat_29819401/article/details/58716834 常见的几种损失函数：https://www.cnblogs.com/hejunlin1992/p/8158933.html 知乎，目标函数，损失函数和代价函数的区别：https://www.zhihu.com/question/52398145 算法常用指标分类常见指标包括AUC，ROC，召回率，准确率.. 回归常见指标包括RMSE… 相关资料： poll的笔记：www.cnblogs.com/maybe2030/p/5375175.html AUC，ROC：https://www.jianshu.com/p/c61ae11cc5f6 特征选择和特征抽取在机器学习中特征是非常重要的，经常需要进行特征工程，因此特征选择和特征抽取相当关键。 特征抽取（Feature Extraction）:Creatting a subset of new features by combinations of the exsiting features.也就是说，特征抽取后的新特征是原来特征的一个映射。 特征选择（Feature Selection）:choosing a subset of all the features(the ones more informative)。也就是说，特征选择后的特征是原来特征的一个子集。 特征抽取算法包括PCA，LDA… 特征选择算法包括三类：Filter, Wrapper, and Embedded 知识点：特征选择和特征抽取的意义，PCA, SVD, … 相关资料： csdn博客：http://blog.csdn.net/shenxiaoming77/article/details/50555054 周志华，机器学习，11章：特征选择和稀疏学习 伯乐在线，主成分分析原理详解：http://blog.jobbole.com/109015/ 连续值，异常值，缺失值处理知识点：连续值转换为离散值，异常值检测，缺失值填充 L1和L2正则化知识点：了解正则化的目标，L1和L2正则的作用，等值线图的含义 相关资料： 博客：http://blog.csdn.net/zouxy09/article/details/24971995 L2正则化导致参数较小：https://blog.csdn.net/jinping_shi/article/details/52433975 等值线图形的解释：https://blog.csdn.net/weixin_39845112/article/details/80114918 过拟合过拟合的一些解决方法： 增加样本数据量； 正则化，L1和L2正则； 神经网络中，dropout方法。就是每层网络的训练，随机的让一半神经元不工作。达到防止过拟合的目的。 决策树中可以用剪枝操作 提升方法：early stopping，当训练集合的误差降低，但是验证集合的误差增加时，则停止训练，同时返回具有最小验证集合误差的神经网络； Data augmentation：创建假数据并增加到训练集中 交叉验证训练集training set：用以建立模型，是用来训练模型或确定模型参数的 测试集testing set: 用来评估模型对未知样本进行预测时的精确度，即泛化能力 验证集validation set: 验证集是原始训练集的子集，用来做模型选择 集成学习集成学习在机器学习中有非常重要的地位，因此需要深入的研究。集成学习重要分为三种，Boosting, Bagging 和Stacking。 Boosting包括了GBDT, XGBoost, AdaBoost… Bagging包含了随机森林 GBDTGBDT中的树都是回归树，不是分类树。 每一轮弱学习器的目标是使预测值接近残差， 强学习器的作用是累加前面所有的弱学习器结果，使预测值接近实际值。 残差是上一轮强学习器的预测结果和实际值的差值或者用损失函数的负梯度方向表示。 Gradient Boosting 是GBDT的核心思想，它相比与 Boosting Tree 的主要改进在于: Boosting Tree 对于每一轮基学习器，拟合的是当前模型与标签值的残差 ， GBDT 对于每一轮基学习器，拟合的是当前模型与标签值的残差的负梯度。 知识点：CART回归树，负梯度，GBDT多分类，GBDT与XGBoost的区别 相关资料： 入门介绍：http://blog.csdn.net/w28971023/article/details/8240756 刘建平：https://www.cnblogs.com/pinard/p/6140514.html 集成学习总结：https://xijunlee.github.io/2017/06/03/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/ GBDT和XGBoost比较：https://blog.csdn.net/panda_zjd/article/details/71577463 面试题整理：https://blog.csdn.net/qq_28031525/article/details/70207918 GBDT构建特征：https://blog.csdn.net/shine19930820/article/details/71713680#2-gbdt构建新的特征思想 GBDT与Boosting Tree的区别：https://blog.csdn.net/Liangjun_Feng/article/details/80142724?spm=5176.11104323.5600004.1.444e717dXEDRaI XGBoost是对GBDT的一种改进，也是实战中常用的模型，因此需要加深理解，建议直接看陈天奇大神的论文，以及他的PPT介绍。 知识点：了解XGBoost模型中各个参数的含义 Lightgbm是另一种常见的模型，由微软提出。 知识点：了解Lightgbm中各个参数的含义，了解lightgbm和xgboost的比较 DART将drop out引入到boosting模型中的一种算法。解决over specilization的问题，即初始的第一棵树对整体方向起关键作用，而后续的树一直处于弥补残差的状态，并且只影响一小部分样本，这样后面的树对整体的贡献也在逐渐的弱化。 相关资料： Csdn: https://blog.csdn.net/Yongchun_Zhu/article/details/78745529 AdaBoostadaboost算法的核心思想就是由分类效果较差的弱分类器逐步的强化成一个分类效果较好的强分类器。而强化的过程，就是逐步的改变样本权重，样本权重的高低，代表其在分类器训练过程中的重要程度。 相关资料： 深度剖析adaboost: http://blog.csdn.net/autocyz/article/details/51305999 一问读懂adaboost: www.xtecher.com/Xfeature/view?aid=8109 随机森林和决策树决策树的关键是选择最优划分属性，了解三种划分属性的方式，如ID3, C4.5, GINI。 Random Forest（随机森林）是Bagging的扩展变体，它在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机特征选择，因此可以概括RF包括四个部分： 1、随机选择样本（放回抽样）； 2、构建决策树（CART树）； 3、随机选择特征； 4、随机森林投票（平均）。 知识点：决策树划分属性的方式，剪枝，Bootstrapping，随机森林进行特征选择 相关资料： 周志华，机器学习； CART回归树, 李航，第五章决策树； 随机森林，面试：https://blog.csdn.net/qq_28031525/article/details/70207918 特征选择：https://blog.csdn.net/banbuduoyujian/article/details/60328474 stacking在kaggle比赛中常用的一种算法，将训练好的所有基模型对整个训练集进行预测，第j个基模型对第i个训练样本的预测值将作为新的训练集中第i个样本的第j个特征值(预测值变成了特征值），最后基于新的训练集进行训练。 相关资料： 从boosting到stacking: https://baijiahao.baidu.com/s?id=1576971033986792968&amp;wfr=spider&amp;for=pc csdn博客，集成学习总结：https://blog.csdn.net/willduan1/article/details/73618677 stack图形说明：https://www.leiphone.com/news/201709/zYIOJqMzR0mJARzj.html Stacking, kaggle: https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard 数据处理数据归一化归一化的作用： 归一化后加快了梯度下降求最优解的速度 归一化有可能提高精度 中心化Zero-centered/Mean-substraction： 指变量减去它的均值 标准化Standardization/Normalization 指变量减去均值后（中心化），再除以标准差； 意义：数据中心化和标准化的作用的取消在回归分析中由于不同特征量纲不同或者数值相差较大所引起的误差。 注意：归一化和标准化都是线性变换。 相关资料： 归一化：http://www.cnblogs.com/LBSer/p/4440590.html Batch Normalization数据预处理，相当于深度学习中的数据归一化。 普通机器学习的归一化只针对输入层，而BN针对中间每一层输入进行归一化。 归一化的目的是使的数据的分布集中在激活函数的敏感区域（即中间区域）。 BN层就是对深度学习的中间层的输入数据进行归一化，解决在训练过程中，中间层数据分布发生改变的情况，使得每一层的分布相似，同时保留原来每层训练得到的特征分布（通过变换重构）。 相关资料： Batch normalization 学习笔记：https://blog.csdn.net/hjimce/article/details/50866313 白化 whitening白化就是深度学习里的PCA. 分为PCA白化和ZCA白化。 白化的目的是去除输入数据的冗余信息。假设训练数据是图像，由于图像中相邻像素之间具有很强的相关性，所以用于训练时输入是冗余的； 白化的目的就是降低输入的冗余性。 白化和PCA的关系： PCA如果不降维，而是仅仅使用PCA求出特征向量，然后把数据X映射到新的特征空间，这样的一个映射过程，其实就是满足了我们白化的第一个性质：除去特征之间的相关性。因此白化算法的实现过程，第一步操作就是PCA，求出新特征空间中X的新坐标，然后再对新的坐标进行方差归一化操作。 相关资料： 白化，hjimce: https://blog.csdn.net/hjimce/article/details/50864602 分类不均衡问题分类不均衡问题是指：分类任务中不同类别的训练样例数目差别很大的情况。 对样例数目较多的类别进行欠抽样（undersampling)（1,2,3均是删去多数类样本的方法）： 随机欠采样：最简单的办法是从多数类中随机抽取样本从而减少多数类样本的数量，使数据达到平衡； Edited Nearest Neighbor(ENN): 在多数类的样本中，如果该样本周围的K近邻样本都是少数类，则将该多数类样本删除； Tomek Link Removal:如果有两个不同类别的样本，它们的最近邻都是对方，那么A,B就是Tomek link。我们要做的就是将所有Tomek link都删除掉，即将组成Tomek link的两个样本，如果有一个属于多数类样本，就将该多数类样本删除掉。 代表性算法EasyEnsemble,利用集成学习机制，将该类别划分为若干个集合供不同学习器使用，对于每个学习器来说都是欠采样，但从全局来看却不会丢失重要信息。 对样例数目较少的类别进行过抽样（oversampling)： 过抽样不是简单的对样本进行重复抽样，否则会招致严重的过拟合，过采样的代表算法是SMOTE,基本思想是对少数类样本进行分析并根据少数类样本人工合成新样本添加到数据集中。简单来说，人工合成的样本是每个少数类样本与其K近邻的连线上的点。 再缩放（再平衡）技术 即给少数类较多的权重，而给多数类更少的权重，使得少数类判别错误的损失大于多数类判别错误的损失，其中，缩放因子为类别样例数目的比值 相关资料： SMOTE算法：https://blog.csdn.net/jiede1/article/details/70215477 分类不均衡：https://blog.csdn.net/ly_ysys629/article/details/72846200 分类不均衡方法综述: http://lib.csdn.net/article/machinelearning/41294]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F08%2F26%2Fhello-world%2F</url>
    <content type="text"><![CDATA[hexo 更新hexo clean hexo g -d]]></content>
      <categories>
        <category>test</category>
      </categories>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
</search>
